Successfully loaded: saved_agents\deep_q_learning\random\random-10000
Iteration: 1, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00974472239613533
Iteration: 2, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007989466190338135
Iteration: 3, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006127919536083937
Iteration: 4, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00039860978722572327
Iteration: 5, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00045183952897787094
Iteration: 6, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005165282636880875
Iteration: 7, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00032215751707553864
Iteration: 8, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013348544016480446
Iteration: 9, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025912243872880936
Iteration: 10, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0056056552566587925
Iteration: 11, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004369331989437342
Iteration: 12, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00399389024823904
Iteration: 13, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029419632628560066
Iteration: 14, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002361895516514778
Iteration: 15, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031088110990822315
Iteration: 16, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004476930480450392
Iteration: 17, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005820899270474911
Iteration: 18, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019056545570492744
Iteration: 19, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004071156959980726
Iteration: 20, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00413003982976079
Iteration: 21, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005307080689817667
Iteration: 22, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005759354680776596
Iteration: 23, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004368153400719166
Iteration: 24, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032303910702466965
Iteration: 25, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0048148054629564285
Iteration: 26, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023579932749271393
Iteration: 27, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028205690905451775
Iteration: 28, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003235042095184326
Iteration: 29, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003896042238920927
Iteration: 30, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007490092888474464
Iteration: 31, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010264323092997074
Iteration: 32, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008078467100858688
Iteration: 33, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005704461596906185
Iteration: 34, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027951733209192753
Iteration: 35, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006006769835948944
Iteration: 36, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003932077903300524
Iteration: 37, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007818452082574368
Iteration: 38, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003533029928803444
Iteration: 39, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013631731271743774
Iteration: 40, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0004774974659085274
Iteration: 41, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00230114022269845
Iteration: 42, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024966495111584663
Iteration: 43, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024986686185002327
Iteration: 44, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003192234318703413
Iteration: 45, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037111560814082623
Iteration: 46, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004557230044156313
Iteration: 47, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004558575805276632
Iteration: 48, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009482222609221935
Iteration: 49, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008634493686258793
Iteration: 50, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007033470086753368
Iteration: 51, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006796890404075384
Iteration: 52, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009423460811376572
Iteration: 53, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005677435547113419
Iteration: 54, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008581088855862617
Perfoming an random action
Iteration: 55, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005941900424659252
Iteration: 56, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033972379751503468
Iteration: 57, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004790022037923336
Iteration: 58, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0041494513861835
Iteration: 59, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034571546129882336
Iteration: 60, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024440442211925983
Iteration: 61, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0003962181508541107
Collision detected, score: 0
Iteration: 62, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0017225295305252075
Iteration: 63, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001619858667254448
Iteration: 64, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00415383093059063
Iteration: 65, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00638301856815815
Iteration: 66, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006342027336359024
Iteration: 67, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 68, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 69, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 70, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 71, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 72, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 73, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 74, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 75, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 76, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 77, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 78, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 79, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 80, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 81, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 82, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 83, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 84, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 85, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008397176861763
Iteration: 86, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025195349007844925
Iteration: 87, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033989385701715946
Collision detected, score: 0
Iteration: 88, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0021700672805309296
Iteration: 89, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000394478440284729
Iteration: 90, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005386034958064556
Iteration: 91, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004693688824772835
Iteration: 92, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010615084320306778
Iteration: 93, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 94, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 95, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 96, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 97, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 98, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 99, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 100, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 101, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 102, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 103, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 104, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 105, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 106, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 107, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 108, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 109, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 110, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 111, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0058633675798773766
Iteration: 112, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004789407830685377
Iteration: 113, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004335916135460138
Iteration: 114, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002399601973593235
Iteration: 115, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034957900643348694
Iteration: 116, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0053724562749266624
Iteration: 117, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003567006438970566
Iteration: 118, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004647546447813511
Iteration: 119, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005915219895541668
Iteration: 120, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004363957326859236
Iteration: 121, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003144068643450737
Iteration: 122, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036300458014011383
Iteration: 123, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0012929756194353104
Iteration: 124, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003910786472260952
Iteration: 125, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003029247745871544
Iteration: 126, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004723730031400919
Iteration: 127, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006892801262438297
Iteration: 128, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006170961074531078
Iteration: 129, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031198719516396523
Iteration: 130, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004054200369864702
Iteration: 131, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031834477558732033
Iteration: 132, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005024515558034182
Iteration: 133, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005192290991544724
Iteration: 134, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030913446098566055
Iteration: 135, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0015832819044589996
Collision detected, score: 0
Iteration: 136, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: -0.0018305890262126923
Iteration: 137, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007355157285928726
Iteration: 138, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007754187565296888
Iteration: 139, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.010190222412347794
Iteration: 140, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005779934581369162
Iteration: 141, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 142, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 143, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 144, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 145, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 146, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 147, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 148, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 149, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 150, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 151, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 152, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 153, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 154, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 155, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 156, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 157, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 158, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 159, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017581060528755188
Iteration: 160, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 8.70879739522934e-05
Iteration: 161, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003593538887798786
Iteration: 162, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028881137259304523
Collision detected, score: 0
Iteration: 163, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0025536641478538513
Iteration: 164, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020190998911857605
Iteration: 165, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006382708437740803
Iteration: 166, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037194332107901573
Iteration: 167, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005795599892735481
Iteration: 168, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 169, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 170, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 171, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 172, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Perfoming an random action
Iteration: 173, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 174, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 175, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Perfoming an random action
Iteration: 176, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 177, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 178, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 179, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 180, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 181, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 182, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 183, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 184, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 185, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 186, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019114790484309196
Iteration: 187, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014735013246536255
Iteration: 188, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008898554369807243
Iteration: 189, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006386330351233482
Iteration: 190, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006592180579900742
Iteration: 191, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006655603647232056
Iteration: 192, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030557396821677685
Iteration: 193, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030117444694042206
Iteration: 194, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024966630153357983
Iteration: 195, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006366925314068794
Iteration: 196, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032870564609766006
Iteration: 197, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005232058465480804
Iteration: 198, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024938806891441345
Iteration: 199, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.583783447742462e-05
Iteration: 200, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0038020936772227287
Iteration: 201, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020216889679431915
Iteration: 202, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022844672203063965
Iteration: 203, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0003057653084397316
Iteration: 204, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002016602084040642
Iteration: 205, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037999670021235943
Iteration: 206, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033980002626776695
Iteration: 207, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007643651217222214
Iteration: 208, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005518287420272827
Iteration: 209, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030275932513177395
Iteration: 210, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002371099777519703
Iteration: 211, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00213688425719738
Iteration: 212, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006721125915646553
Iteration: 213, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009222092106938362
Iteration: 214, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039461092092096806
Iteration: 215, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012980606406927109
Iteration: 216, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024649309925734997
Iteration: 217, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012738490477204323
Iteration: 218, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005183128640055656
Iteration: 219, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005088718608021736
Iteration: 220, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006169255822896957
Iteration: 221, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004213147796690464
Iteration: 222, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004896866157650948
Iteration: 223, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020041577517986298
Iteration: 224, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002911727875471115
Iteration: 225, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00224970281124115
Iteration: 226, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017252825200557709
Iteration: 227, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004071120172739029
Iteration: 228, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011468036100268364
Iteration: 229, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00817786157131195
Collision detected, score: 0
Iteration: 230, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.006375714670866728
Iteration: 231, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042568896897137165
Iteration: 232, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011086245067417622
Iteration: 233, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007955116219818592
Iteration: 234, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007504034321755171
Iteration: 235, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 236, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 237, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 238, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 239, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 240, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 241, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 242, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 243, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 244, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 245, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 246, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 247, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 248, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 249, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 250, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 251, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 252, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 253, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019361022859811783
Iteration: 254, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012499121949076653
Iteration: 255, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00042262114584445953
Iteration: 256, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006680488586425781
Iteration: 257, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0003489060327410698
Iteration: 258, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0002294117584824562
Iteration: 259, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 8.770078420639038e-05
Iteration: 260, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004756313748657703
Iteration: 261, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002307687420397997
Iteration: 262, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012379470281302929
Iteration: 263, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010623903013765812
Iteration: 264, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008860443718731403
Iteration: 265, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009655941277742386
Iteration: 266, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010467835702002048
Iteration: 267, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011889724060893059
Iteration: 268, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010860647074878216
Iteration: 269, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008036982268095016
Iteration: 270, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009356552734971046
Iteration: 271, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004327078815549612
Iteration: 272, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005617205984890461
Iteration: 273, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0074363527819514275
Iteration: 274, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.008157468400895596
Iteration: 275, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005679704248905182
Iteration: 276, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007305670529603958
Iteration: 277, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005138695240020752
Iteration: 278, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028052031993865967
Iteration: 279, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006363320164382458
Iteration: 280, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008321141824126244
Iteration: 281, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00455744843930006
Iteration: 282, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023748651146888733
Iteration: 283, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035302299074828625
Iteration: 284, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001527196727693081
Iteration: 285, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007981942966580391
Iteration: 286, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00693123834207654
Iteration: 287, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005705615971237421
Iteration: 288, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00575631856918335
Iteration: 289, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004029311239719391
Iteration: 290, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001377081498503685
Iteration: 291, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0016535939648747444
Collision detected, score: 0
Iteration: 292, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: -0.0017477385699748993
Iteration: 293, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002869244199246168
Iteration: 294, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033496245741844177
Iteration: 295, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031125503592193127
Iteration: 296, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000791509635746479
Iteration: 297, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 298, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017140824347734451
Iteration: 299, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016389107331633568
Iteration: 300, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014947187155485153
Iteration: 301, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001966129057109356
Iteration: 302, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004209826700389385
Iteration: 303, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004745318088680506
Iteration: 304, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003355017863214016
Iteration: 305, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004265457391738892
Iteration: 306, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004399097058922052
Iteration: 307, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004322832450270653
Iteration: 308, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019652200862765312
Iteration: 309, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001681785099208355
Iteration: 310, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006626388058066368
Iteration: 311, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003104171250015497
Iteration: 312, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034972475841641426
Iteration: 313, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008184014819562435
Iteration: 314, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00528718251734972
Iteration: 315, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005066887475550175
Iteration: 316, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036969278007745743
Iteration: 317, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032563675194978714
Iteration: 318, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005653563421219587
Iteration: 319, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00044756848365068436
Iteration: 320, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00599070405587554
Iteration: 321, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031554559245705605
Iteration: 322, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017767753452062607
Iteration: 323, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002001240849494934
Iteration: 324, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006665568798780441
Iteration: 325, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009644188918173313
Iteration: 326, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008963420987129211
Iteration: 327, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008126309141516685
Iteration: 328, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008200148120522499
Iteration: 329, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004064289852976799
Iteration: 330, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032232189550995827
Iteration: 331, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0054972125217318535
Iteration: 332, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0053073447197675705
Iteration: 333, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005358611233532429
Iteration: 334, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005018345080316067
Iteration: 335, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007669366896152496
Iteration: 336, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00988149456679821
Iteration: 337, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010264609009027481
Iteration: 338, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010195877403020859
Iteration: 339, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0077173421159386635
Iteration: 340, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009418793022632599
Iteration: 341, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009363709948956966
Iteration: 342, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011666171252727509
Iteration: 343, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013449311256408691
Iteration: 344, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011682277545332909
Iteration: 345, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006856357678771019
Iteration: 346, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006716539617627859
Iteration: 347, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005382495000958443
Iteration: 348, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001085604541003704
Iteration: 349, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00204506516456604
Iteration: 350, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002107265405356884
Iteration: 351, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.000424262136220932
Iteration: 352, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010534292086958885
Iteration: 353, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009132111445069313
Iteration: 354, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000925273634493351
Collision detected, score: 0
Iteration: 355, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0008552549406886101
Iteration: 356, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002992392983287573
Iteration: 357, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004828161094337702
Iteration: 358, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004868540912866592
Iteration: 359, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013740435242652893
Iteration: 360, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 361, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 362, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 363, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 364, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 365, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 366, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 367, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 368, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 369, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 370, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 371, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 372, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 373, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 374, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 375, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 376, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 377, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 378, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008215587586164474
Iteration: 379, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002650063019245863
Iteration: 380, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032404903322458267
Collision detected, score: 0
Iteration: 381, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0017106086015701294
Iteration: 382, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005072401836514473
Iteration: 383, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007761528715491295
Iteration: 384, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005794116295874119
Iteration: 385, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0054343827068805695
Iteration: 386, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 387, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 388, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 389, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 390, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 391, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 392, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 393, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 394, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 395, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Perfoming an random action
Iteration: 396, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 397, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 398, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 399, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 400, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 401, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 402, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 403, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 404, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00159358698874712
Iteration: 405, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001845979131758213
Iteration: 406, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014078672975301743
Iteration: 407, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0002834480255842209
Iteration: 408, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007345099002122879
Iteration: 409, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -6.200186908245087e-05
Iteration: 410, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0039116377010941505
Iteration: 411, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005315078888088465
Iteration: 412, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023862365633249283
Iteration: 413, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007981310598552227
Iteration: 414, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00658093485981226
Iteration: 415, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040213256143033504
Iteration: 416, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008839837275445461
Iteration: 417, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006049053277820349
Iteration: 418, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007304821629077196
Iteration: 419, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005925246514379978
Iteration: 420, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007533176802098751
Iteration: 421, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009363967925310135
Iteration: 422, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007196775171905756
Iteration: 423, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009839823469519615
Iteration: 424, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007751277182251215
Iteration: 425, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0058923810720443726
Iteration: 426, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005664423108100891
Iteration: 427, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005873639602214098
Iteration: 428, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00530417961999774
Iteration: 429, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006664035841822624
Iteration: 430, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006820846814662218
Iteration: 431, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005260252859443426
Iteration: 432, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004333776421844959
Iteration: 433, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -3.864988684654236e-07
Iteration: 434, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014689266681671143
Iteration: 435, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0012384839355945587
Iteration: 436, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001779484562575817
Iteration: 437, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.001278989017009735
Iteration: 438, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023567331954836845
Iteration: 439, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012862132862210274
Iteration: 440, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028053363785147667
Iteration: 441, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002006927505135536
Iteration: 442, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002177673391997814
Iteration: 443, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008630687370896339
Iteration: 444, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00019387342035770416
Iteration: 445, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014593061059713364
Iteration: 446, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032320283353328705
Iteration: 447, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002763926051557064
Iteration: 448, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0002525597810745239
Iteration: 449, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00269906222820282
Collision detected, score: 0
Iteration: 450, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.002565835602581501
Iteration: 451, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003204471431672573
Iteration: 452, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00678782444447279
Iteration: 453, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0074311429634690285
Iteration: 454, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 1.2285076081752777e-05
Iteration: 455, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 456, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 457, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 458, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 459, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 460, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 461, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 462, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 463, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 464, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 465, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 466, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 467, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 468, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 469, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 470, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 471, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 472, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 473, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015996722504496574
Iteration: 474, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012713121250271797
Iteration: 475, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 2.2822991013526917e-05
Iteration: 476, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007187789306044579
Iteration: 477, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00025085173547267914
Iteration: 478, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0009098630398511887
Iteration: 479, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001519463025033474
Iteration: 480, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002511607948690653
Iteration: 481, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002692887559533119
Iteration: 482, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011820642277598381
Iteration: 483, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006544320844113827
Iteration: 484, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563901849091053
Iteration: 485, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007921667769551277
Iteration: 486, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006394724361598492
Iteration: 487, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007878702133893967
Iteration: 488, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008237733505666256
Iteration: 489, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009070334956049919
Iteration: 490, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007128153927624226
Iteration: 491, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006478023715317249
Iteration: 492, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007206307724118233
Iteration: 493, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004764017648994923
Iteration: 494, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008992905728518963
Iteration: 495, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010206624865531921
Iteration: 496, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00769373495131731
Iteration: 497, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007462593726813793
Iteration: 498, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006987493485212326
Iteration: 499, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008123734034597874
Iteration: 500, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005258691962808371
Iteration: 501, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002837284468114376
Iteration: 502, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00232072826474905
Iteration: 503, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007824655622243881
Iteration: 504, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006365061737596989
Iteration: 505, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009360406547784805
Iteration: 506, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006054823752492666
Iteration: 507, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00782494805753231
Iteration: 508, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004467662423849106
Iteration: 509, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006472252309322357
Iteration: 510, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002700353506952524
Iteration: 511, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005262475460767746
Collision detected, score: 0
Iteration: 512, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0012156115844845772
Iteration: 513, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014093797653913498
Iteration: 514, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005422572605311871
Iteration: 515, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022561615332961082
Iteration: 516, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0001249769702553749
Iteration: 517, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 518, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 519, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 520, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 521, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 522, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 523, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 524, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 525, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 526, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 527, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 528, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 529, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 530, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 531, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 532, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 533, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 534, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 535, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000674479641020298
Iteration: 536, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029716743156313896
Iteration: 537, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025010276585817337
Collision detected, score: 0
Iteration: 538, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.001746663823723793
Iteration: 539, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005621584132313728
Iteration: 540, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005287325941026211
Iteration: 541, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005967940669506788
Iteration: 542, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006746184080839157
Iteration: 543, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 544, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 545, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 546, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 547, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 548, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 549, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 550, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 551, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 552, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 553, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 554, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 555, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 556, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 557, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 558, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 559, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 560, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 561, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005624123848974705
Iteration: 562, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004364904947578907
Iteration: 563, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004658166319131851
Iteration: 564, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036840341053903103
Iteration: 565, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003028706181794405
Iteration: 566, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005212730262428522
Iteration: 567, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 3.3505260944366455e-05
Iteration: 568, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005335979163646698
Iteration: 569, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004951305687427521
Iteration: 570, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005687282420694828
Iteration: 571, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003799966536462307
Iteration: 572, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002533932216465473
Iteration: 573, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00265638530254364
Iteration: 574, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006367695517838001
Iteration: 575, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007748347707092762
Iteration: 576, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0054168784990906715
Iteration: 577, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004370234906673431
Iteration: 578, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006506459787487984
Iteration: 579, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00811805296689272
Iteration: 580, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006758234463632107
Iteration: 581, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004188250284641981
Iteration: 582, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006527657620608807
Iteration: 583, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0040259528905153275
Iteration: 584, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016662608832120895
Iteration: 585, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003580265212804079
Iteration: 586, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -8.903257548809052e-05
Iteration: 587, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 2.714991569519043e-05
Iteration: 588, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002615779172629118
Iteration: 589, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008533615618944168
Iteration: 590, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009703375399112701
Iteration: 591, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00046618934720754623
Iteration: 592, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0045629264786839485
Iteration: 593, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005938977003097534
Iteration: 594, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006560172885656357
Iteration: 595, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009880099445581436
Iteration: 596, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008147921413183212
Iteration: 597, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003997208550572395
Iteration: 598, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00135781429708004
Iteration: 599, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020712288096547127
Collision detected, score: 0
Iteration: 600, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.004422382917255163
Iteration: 601, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004467297811061144
Iteration: 602, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00700758583843708
Iteration: 603, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00475089717656374
Iteration: 604, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005368357058614492
Iteration: 605, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 606, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 607, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 608, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 609, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 610, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 611, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 612, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 613, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 614, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 615, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 616, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 617, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 618, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 619, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 620, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 621, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 622, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 623, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002001662738621235
Iteration: 624, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010593878105282784
Iteration: 625, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007365886121988297
Iteration: 626, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0001870039850473404
Iteration: 627, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00026066601276397705
Iteration: 628, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.000898534432053566
Iteration: 629, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023226384073495865
Iteration: 630, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037346119061112404
Iteration: 631, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037356908433139324
Iteration: 632, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007639684248715639
Iteration: 633, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007019295357167721
Iteration: 634, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006833411753177643
Iteration: 635, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007895015180110931
Iteration: 636, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005437465850263834
Iteration: 637, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007223308086395264
Iteration: 638, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002781820483505726
Iteration: 639, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005455043166875839
Iteration: 640, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0051775285974144936
Iteration: 641, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032776780426502228
Iteration: 642, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004561509005725384
Iteration: 643, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0052076843567192554
Iteration: 644, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005289811175316572
Iteration: 645, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005671836901456118
Iteration: 646, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006390717811882496
Iteration: 647, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004314810037612915
Iteration: 648, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043705725111067295
Iteration: 649, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006268656346946955
Iteration: 650, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007432871963828802
Iteration: 651, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004408782348036766
Iteration: 652, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004486548714339733
Iteration: 653, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005008013918995857
Iteration: 654, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007953613996505737
Iteration: 655, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012722323648631573
Iteration: 656, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009785927832126617
Iteration: 657, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014303907752037048
Iteration: 658, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008779584430158138
Iteration: 659, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012358073145151138
Iteration: 660, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005458472296595573
Iteration: 661, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00028607528656721115
Collision detected, score: 0
Iteration: 662, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.001192745752632618
Iteration: 663, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004585748538374901
Iteration: 664, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007938234135508537
Iteration: 665, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001188153401017189
Iteration: 666, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010520759969949722
Iteration: 667, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 668, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017140824347734451
Iteration: 669, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016389107331633568
Iteration: 670, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014947187155485153
Iteration: 671, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001966129057109356
Iteration: 672, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004209826700389385
Iteration: 673, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004745318088680506
Iteration: 674, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003355017863214016
Iteration: 675, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004265457391738892
Iteration: 676, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004399097058922052
Iteration: 677, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004322832450270653
Iteration: 678, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019652200862765312
Iteration: 679, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001681785099208355
Iteration: 680, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006626388058066368
Iteration: 681, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003104171250015497
Iteration: 682, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034972475841641426
Iteration: 683, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008184014819562435
Iteration: 684, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00528718251734972
Iteration: 685, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0048381430096924305
Iteration: 686, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038622627034783363
Iteration: 687, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003546061459928751
Iteration: 688, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004506579600274563
Iteration: 689, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002147437073290348
Iteration: 690, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005617564544081688
Iteration: 691, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00672536576166749
Iteration: 692, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036299536004662514
Iteration: 693, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00044663622975349426
Iteration: 694, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005012059584259987
Iteration: 695, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004975572694092989
Iteration: 696, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005227966699749231
Iteration: 697, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004676226060837507
Iteration: 698, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032734437845647335
Iteration: 699, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009697768837213516
Iteration: 700, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023002708330750465
Iteration: 701, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011321930214762688
Iteration: 702, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002271677367389202
Iteration: 703, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004803275223821402
Iteration: 704, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0035341335460543633
Iteration: 705, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003720746375620365
Iteration: 706, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009310441091656685
Iteration: 707, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006222633644938469
Iteration: 708, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006729088723659515
Iteration: 709, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005459671840071678
Iteration: 710, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00830408837646246
Iteration: 711, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006787565071135759
Iteration: 712, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003815703559666872
Iteration: 713, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006741037126630545
Collision detected, score: 0
Iteration: 714, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0018713222816586494
Iteration: 715, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012588761746883392
Iteration: 716, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009101593866944313
Iteration: 717, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003532834816724062
Iteration: 718, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034154276363551617
Iteration: 719, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 720, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 721, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 722, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 723, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 724, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 725, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 726, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 727, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 728, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 729, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 730, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 731, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 732, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 733, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 734, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 735, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 736, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 737, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001526092179119587
Iteration: 738, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000668354332447052
Iteration: 739, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003425267059355974
Iteration: 740, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002090718597173691
Collision detected, score: 0
Iteration: 741, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0012266580015420914
Iteration: 742, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00272600119933486
Iteration: 743, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027175243012607098
Iteration: 744, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004552321508526802
Iteration: 745, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00018589850515127182
Iteration: 746, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 747, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -9.88394021987915e-05
Iteration: 748, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00020933710038661957
Iteration: 749, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012660948559641838
Iteration: 750, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018334342166781425
Iteration: 751, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0044052572920918465
Iteration: 752, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005087495781481266
Iteration: 753, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028295740485191345
Iteration: 754, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036235665902495384
Iteration: 755, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011557694524526596
Iteration: 756, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002689965069293976
Iteration: 757, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00020991358906030655
Iteration: 758, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00014249607920646667
Iteration: 759, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002909666858613491
Iteration: 760, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013370774686336517
Iteration: 761, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003773805685341358
Iteration: 762, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002132318913936615
Iteration: 763, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0001215711236000061
Iteration: 764, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022874660789966583
Iteration: 765, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023893285542726517
Collision detected, score: 0
Iteration: 766, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0022404268383979797
Iteration: 767, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035374597646296024
Iteration: 768, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005416827742010355
Iteration: 769, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004532357212156057
Iteration: 770, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007181604392826557
Iteration: 771, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 772, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 773, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 774, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 775, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 776, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 777, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 778, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 779, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 780, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 781, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 782, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 783, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 784, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 785, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 786, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 787, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 788, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 789, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015125228092074394
Iteration: 790, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -1.9935891032218933e-05
Iteration: 791, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003484605811536312
Iteration: 792, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00303090363740921
Collision detected, score: 0
Iteration: 793, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.0028023389168083668
Iteration: 794, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024239616468548775
Iteration: 795, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006132674869149923
Iteration: 796, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004229801706969738
Iteration: 797, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004708077758550644
Iteration: 798, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 799, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 800, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 801, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 802, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 803, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 804, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 805, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 806, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 807, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 808, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 809, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 810, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 811, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 812, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 813, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 814, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 815, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 816, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005947108846157789
Iteration: 817, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00490277074277401
Iteration: 818, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00404680659994483
Iteration: 819, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003843147773295641
Iteration: 820, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004441365599632263
Iteration: 821, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007570861838757992
Iteration: 822, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014348030090332031
Iteration: 823, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005255861207842827
Iteration: 824, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0072655389085412025
Iteration: 825, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009727858938276768
Iteration: 826, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009007749147713184
Iteration: 827, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006848587654531002
Iteration: 828, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005783042870461941
Iteration: 829, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006680076010525227
Iteration: 830, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00950892735272646
Iteration: 831, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00996851734817028
Iteration: 832, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008313694968819618
Iteration: 833, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009939402341842651
Iteration: 834, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009401174262166023
Iteration: 835, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009655900299549103
Iteration: 836, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011187355034053326
Iteration: 837, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009889916516840458
Iteration: 838, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008801368065178394
Iteration: 839, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012779796496033669
Iteration: 840, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01129989419132471
Iteration: 841, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009608357213437557
Iteration: 842, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012746954336762428
Iteration: 843, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011277807876467705
Iteration: 844, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004663728177547455
Iteration: 845, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006050199270248413
Iteration: 846, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0045663174241781235
Iteration: 847, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017951056361198425
Iteration: 848, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00700658094137907
Iteration: 849, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033990521915256977
Iteration: 850, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002246825024485588
Iteration: 851, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011026998981833458
Iteration: 852, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005606448277831078
Iteration: 853, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005124323535710573
Iteration: 854, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036183418706059456
Iteration: 855, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0015468783676624298
Iteration: 856, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0050085606053471565
Iteration: 857, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036600041203200817
Iteration: 858, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0004410305991768837
Iteration: 859, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042833127081394196
Iteration: 860, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -6.422586739063263e-05
Collision detected, score: 0
Iteration: 861, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00463767210021615
Iteration: 862, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004292979836463928
Iteration: 863, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0028213877230882645
Iteration: 864, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005675542633980513
Iteration: 865, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005264738574624062
Iteration: 866, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 867, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 868, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 869, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 870, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 871, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 872, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 873, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 874, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Perfoming an random action
Iteration: 875, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 876, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 877, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Perfoming an random action
Iteration: 878, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 879, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 880, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 881, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 882, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 883, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 884, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000918869860470295
Iteration: 885, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024743452668190002
Iteration: 886, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036914078518748283
Collision detected, score: 0
Iteration: 887, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0026582544669508934
Iteration: 888, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013316972181200981
Iteration: 889, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007641723845154047
Iteration: 890, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005012462846934795
Iteration: 891, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005493575241416693
Iteration: 892, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 893, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 894, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 895, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 896, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 897, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 898, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 899, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 900, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 901, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 902, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 903, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 904, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 905, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 906, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 907, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 908, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 909, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 910, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017676809802651405
Iteration: 911, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024338411167263985
Iteration: 912, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015020277351140976
Iteration: 913, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009316382929682732
Perfoming an random action
Iteration: 914, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000357605516910553
Iteration: 915, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005586985498666763
Iteration: 916, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014428719878196716
Iteration: 917, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019331835210323334
Iteration: 918, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022714724764227867
Iteration: 919, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010699361562728882
Iteration: 920, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0098269023001194
Iteration: 921, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008271576836705208
Iteration: 922, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00994080025702715
Iteration: 923, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007291259244084358
Iteration: 924, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006827194709330797
Iteration: 925, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0062957932241261005
Iteration: 926, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008625766262412071
Iteration: 927, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014156028628349304
Iteration: 928, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01254056766629219
Iteration: 929, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01249607838690281
Iteration: 930, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012184513732790947
Iteration: 931, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014633923768997192
Iteration: 932, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012081059627234936
Iteration: 933, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012012027204036713
Iteration: 934, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010973640717566013
Iteration: 935, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006827706471085548
Iteration: 936, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006354162469506264
Iteration: 937, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0064013623632490635
Iteration: 938, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006137371063232422
Iteration: 939, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020479243248701096
Iteration: 940, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.004555889405310154
Iteration: 941, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.004854151979088783
Iteration: 942, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.001936454325914383
Iteration: 943, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0008630761876702309
Iteration: 944, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0020636552944779396
Iteration: 945, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000337209552526474
Iteration: 946, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.000675031915307045
Iteration: 947, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00025319959968328476
Iteration: 948, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0008185263723134995
Iteration: 949, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -2.543814480304718e-05
Iteration: 950, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005291676614433527
Iteration: 951, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00529493298381567
Iteration: 952, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018482133746147156
Iteration: 953, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010994803160429
Iteration: 954, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0017118770629167557
Iteration: 955, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037640612572431564
Iteration: 956, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0049727968871593475
Collision detected, score: 0
Iteration: 957, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.001348833553493023
Iteration: 958, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032701329328119755
Iteration: 959, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006017245817929506
Iteration: 960, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008753628470003605
Iteration: 961, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006720392033457756
Iteration: 962, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 963, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 964, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 965, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 966, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 967, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 968, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 969, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 970, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 971, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 972, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 973, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 974, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 975, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 976, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 977, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 978, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 979, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 980, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018062181770801544
Iteration: 981, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017812466248869896
Iteration: 982, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006678253412246704
Iteration: 983, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007416354492306709
Iteration: 984, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.231238901615143e-05
Iteration: 985, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009696194902062416
Iteration: 986, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012078871950507164
Iteration: 987, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000988808460533619
Iteration: 988, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031441356986761093
Iteration: 989, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007277422584593296
Iteration: 990, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009626835584640503
Iteration: 991, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0067928628996014595
Iteration: 992, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006065732333809137
Iteration: 993, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011913604103028774
Iteration: 994, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010889807716012001
Iteration: 995, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010500788688659668
Iteration: 996, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007326423190534115
Iteration: 997, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011480476707220078
Iteration: 998, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007850504480302334
Iteration: 999, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010859300382435322
Iteration: 1000, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010164393112063408
Iteration: 1001, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009597346186637878
Iteration: 1002, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00868457555770874
Iteration: 1003, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008441135287284851
Iteration: 1004, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0077204410918056965
Iteration: 1005, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004723802208900452
Iteration: 1006, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004543464630842209
Iteration: 1007, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036259209737181664
Iteration: 1008, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003779064863920212
Iteration: 1009, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 5.156639963388443e-05
Iteration: 1010, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002939297817647457
Iteration: 1011, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0038872957229614258
Collision detected, score: 0
Iteration: 1012, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0025689173489809036
Iteration: 1013, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002358171623200178
Iteration: 1014, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010782245546579361
Iteration: 1015, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022146226838231087
Iteration: 1016, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006692786701023579
Iteration: 1017, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1018, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1019, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1020, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1021, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1022, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1023, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1024, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1025, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1026, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1027, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1028, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1029, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1030, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1031, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1032, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1033, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1034, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1035, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015294244512915611
Iteration: 1036, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017284462228417397
Iteration: 1037, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00015972740948200226
Iteration: 1038, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00027046725153923035
Iteration: 1039, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011511826887726784
Iteration: 1040, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005334196612238884
Iteration: 1041, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003528295550495386
Iteration: 1042, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0038860621862113476
Iteration: 1043, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033678337931632996
Iteration: 1044, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004893110599368811
Iteration: 1045, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006989823654294014
Iteration: 1046, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004268806427717209
Iteration: 1047, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033465265296399593
Iteration: 1048, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004490838386118412
Iteration: 1049, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018113525584340096
Iteration: 1050, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005511168856173754
Iteration: 1051, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005919894203543663
Iteration: 1052, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00532170245423913
Iteration: 1053, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019115209579467773
Iteration: 1054, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00237894244492054
Iteration: 1055, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004184430465102196
Iteration: 1056, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003848315216600895
Iteration: 1057, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005611628293991089
Iteration: 1058, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0049777342937886715
Iteration: 1059, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0022578840143978596
Iteration: 1060, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010809944942593575
Iteration: 1061, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0068717291578650475
Iteration: 1062, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005616980604827404
Iteration: 1063, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026987148448824883
Iteration: 1064, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037841093726456165
Iteration: 1065, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025762515142560005
Iteration: 1066, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004711887799203396
Iteration: 1067, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039976732805371284
Iteration: 1068, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0068768528290092945
Iteration: 1069, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028947480022907257
Iteration: 1070, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0067074974067509174
Iteration: 1071, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00277902465313673
Iteration: 1072, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006916938815265894
Iteration: 1073, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013968264684081078
Iteration: 1074, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00551840104162693
Collision detected, score: 0
Iteration: 1075, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.005485509522259235
Iteration: 1076, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004480439703911543
Iteration: 1077, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009174561128020287
Iteration: 1078, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005368199199438095
Iteration: 1079, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004301778040826321
Iteration: 1080, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1081, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1082, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1083, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1084, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1085, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1086, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1087, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1088, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1089, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1090, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1091, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1092, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1093, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1094, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1095, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1096, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1097, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1098, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020693503320217133
Iteration: 1099, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019863126799464226
Iteration: 1100, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00039869919419288635
Iteration: 1101, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006169555708765984
Iteration: 1102, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008986899629235268
Iteration: 1103, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000660027377307415
Iteration: 1104, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0004184301942586899
Iteration: 1105, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0003965795040130615
Iteration: 1106, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028560669161379337
Iteration: 1107, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006557447370141745
Iteration: 1108, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009057661518454552
Iteration: 1109, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0051656123250722885
Iteration: 1110, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005476136691868305
Iteration: 1111, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01117443386465311
Iteration: 1112, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010054479353129864
Iteration: 1113, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010213867761194706
Iteration: 1114, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005860925652086735
Iteration: 1115, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010918946005403996
Iteration: 1116, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0074263326823711395
Iteration: 1117, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011305464431643486
Iteration: 1118, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010753411799669266
Iteration: 1119, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01191350445151329
Iteration: 1120, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008953255601227283
Iteration: 1121, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00972466915845871
Iteration: 1122, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007678608410060406
Iteration: 1123, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002606075257062912
Iteration: 1124, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00478536169975996
Iteration: 1125, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00282121729105711
Iteration: 1126, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011742385104298592
Iteration: 1127, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 9.656231850385666e-05
Iteration: 1128, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0028241677209734917
Iteration: 1129, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003592587076127529
Collision detected, score: 0
Iteration: 1130, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0014674104750156403
Iteration: 1131, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030843252316117287
Iteration: 1132, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00954018160700798
Iteration: 1133, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002229833509773016
Iteration: 1134, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006572175770998001
Iteration: 1135, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 1136, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 1137, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 1138, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 1139, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 1140, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 1141, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 1142, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 1143, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Perfoming an random action
Iteration: 1144, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 1145, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 1146, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021640602499246597
Iteration: 1147, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0018979525193572044
Iteration: 1148, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001660686917603016
Iteration: 1149, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001624109223484993
Iteration: 1150, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007040929049253464
Iteration: 1151, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018768450245261192
Iteration: 1152, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001470637507736683
Iteration: 1153, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010640164837241173
Iteration: 1154, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003317978698760271
Iteration: 1155, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009677466005086899
Iteration: 1156, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003317382652312517
Iteration: 1157, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029852939769625664
Collision detected, score: 0
Iteration: 1158, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0010557584464550018
Iteration: 1159, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018771374598145485
Iteration: 1160, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008343587629497051
Iteration: 1161, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004817703273147345
Iteration: 1162, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007424661889672279
Iteration: 1163, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1164, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1165, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1166, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1167, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1168, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1169, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1170, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1171, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1172, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1173, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1174, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1175, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1176, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1177, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1178, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1179, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1180, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1181, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001695837825536728
Iteration: 1182, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019455226138234138
Iteration: 1183, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005723787471652031
Iteration: 1184, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000981832854449749
Iteration: 1185, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00012786593288183212
Iteration: 1186, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00042400602251291275
Iteration: 1187, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024108835496008396
Iteration: 1188, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00468220841139555
Iteration: 1189, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030034156516194344
Iteration: 1190, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008309047669172287
Iteration: 1191, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007351802196353674
Perfoming an random action
Iteration: 1192, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005269214510917664
Iteration: 1193, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008442058227956295
Iteration: 1194, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007589900866150856
Iteration: 1195, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008094476535916328
Iteration: 1196, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007790375966578722
Iteration: 1197, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006993783637881279
Iteration: 1198, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006553760729730129
Iteration: 1199, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009212538599967957
Iteration: 1200, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010184532031416893
Iteration: 1201, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008185327984392643
Iteration: 1202, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006711872294545174
Iteration: 1203, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006024409085512161
Iteration: 1204, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004456197377294302
Iteration: 1205, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004498105496168137
Iteration: 1206, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0055794622749090195
Iteration: 1207, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0061227111145854
Iteration: 1208, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004040033556520939
Iteration: 1209, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005086233373731375
Iteration: 1210, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007801670581102371
Iteration: 1211, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012650620192289352
Iteration: 1212, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00016978848725557327
Iteration: 1213, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 2.81650573015213e-05
Iteration: 1214, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010595470666885376
Iteration: 1215, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004471599590033293
Iteration: 1216, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002465468365699053
Iteration: 1217, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008040834218263626
Iteration: 1218, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011352049186825752
Iteration: 1219, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0029544662684202194
Iteration: 1220, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0049303071573376656
Iteration: 1221, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0029264185577630997
Iteration: 1222, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00014120619744062424
Iteration: 1223, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001014593057334423
Iteration: 1224, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0017303647473454475
Iteration: 1225, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0036034872755408287
Iteration: 1226, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0031377598643302917
Iteration: 1227, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0016321074217557907
Iteration: 1228, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002375531941652298
Iteration: 1229, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029755677096545696
Iteration: 1230, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036651580594480038
Iteration: 1231, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003966870717704296
Iteration: 1232, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00044034700840711594
Iteration: 1233, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001168200746178627
Iteration: 1234, learning state: observe, epsilon: 0.0075, did jump: No, reward: 1, Q_max: 0.003949850331991911
Iteration: 1235, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -9.499862790107727e-05
Iteration: 1236, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002702224999666214
Collision detected, score: 0
Iteration: 1237, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00254348898306489
Iteration: 1238, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00221536448225379
Iteration: 1239, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010649669915437698
Iteration: 1240, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012670431286096573
Iteration: 1241, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007487027905881405
Iteration: 1242, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1243, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00330183282494545
Iteration: 1244, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002642674371600151
Iteration: 1245, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0009143836796283722
Iteration: 1246, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008190767839550972
Iteration: 1247, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007849046960473061
Iteration: 1248, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001066630706191063
Iteration: 1249, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 5.462300032377243e-05
Iteration: 1250, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002236592583358288
Iteration: 1251, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037690591998398304
Iteration: 1252, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005692086648195982
Iteration: 1253, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004414697643369436
Iteration: 1254, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023437347263097763
Iteration: 1255, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002179504372179508
Iteration: 1256, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0002999426797032356
Iteration: 1257, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002160165458917618
Iteration: 1258, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007373783737421036
Iteration: 1259, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002501250244677067
Iteration: 1260, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008451817557215691
Iteration: 1261, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001706935465335846
Iteration: 1262, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029406300745904446
Iteration: 1263, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019504865631461143
Iteration: 1264, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030840118415653706
Iteration: 1265, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004522975534200668
Iteration: 1266, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002488476689904928
Iteration: 1267, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004228617530316114
Iteration: 1268, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004795645363628864
Iteration: 1269, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003431019838899374
Iteration: 1270, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007609236054122448
Iteration: 1271, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003577984869480133
Collision detected, score: 0
Iteration: 1272, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.005093980580568314
Iteration: 1273, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012063445523381233
Iteration: 1274, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010453744791448116
Iteration: 1275, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004819809924811125
Iteration: 1276, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005438387859612703
Iteration: 1277, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1278, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1279, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1280, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1281, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1282, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1283, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1284, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1285, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1286, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1287, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1288, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1289, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1290, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1291, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1292, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1293, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1294, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1295, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023115966469049454
Iteration: 1296, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00260300375521183
Iteration: 1297, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014049336314201355
Iteration: 1298, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014925869181752205
Iteration: 1299, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0002421317622065544
Iteration: 1300, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002179434522986412
Iteration: 1301, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012603942304849625
Perfoming an random action
Iteration: 1302, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0027793017216026783
Iteration: 1303, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003161356318742037
Iteration: 1304, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010511954315006733
Iteration: 1305, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008036533370614052
Iteration: 1306, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00891684740781784
Iteration: 1307, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010323354043066502
Iteration: 1308, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009683617390692234
Iteration: 1309, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009561276994645596
Iteration: 1310, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012133331038057804
Iteration: 1311, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01254032552242279
Iteration: 1312, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01168543566018343
Iteration: 1313, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00577132822945714
Iteration: 1314, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006040400825440884
Iteration: 1315, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028933207504451275
Iteration: 1316, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005355262663215399
Iteration: 1317, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00525700394064188
Iteration: 1318, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00488516828045249
Iteration: 1319, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004034742712974548
Iteration: 1320, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000720558688044548
Iteration: 1321, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023482153192162514
Iteration: 1322, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017354190349578857
Iteration: 1323, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003500669728964567
Iteration: 1324, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023452527821063995
Iteration: 1325, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019558072090148926
Iteration: 1326, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000582745298743248
Collision detected, score: 0
Iteration: 1327, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.001479174941778183
Iteration: 1328, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003031475003808737
Iteration: 1329, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006821762304753065
Iteration: 1330, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006006121169775724
Iteration: 1331, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00043535977602005005
Iteration: 1332, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0020505720749497414
Iteration: 1333, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00046829134225845337
Iteration: 1334, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031136274337768555
Iteration: 1335, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000613001175224781
Iteration: 1336, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023591392673552036
Iteration: 1337, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002512422390282154
Iteration: 1338, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0058829947374761105
Iteration: 1339, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005641715135425329
Iteration: 1340, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003372275736182928
Iteration: 1341, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018994547426700592
Iteration: 1342, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00046866852790117264
Iteration: 1343, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005603889003396034
Iteration: 1344, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002648182213306427
Iteration: 1345, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002871586475521326
Iteration: 1346, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005262645892798901
Iteration: 1347, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024583195336163044
Iteration: 1348, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0023977551609277725
Iteration: 1349, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016443915665149689
Iteration: 1350, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000970437191426754
Iteration: 1351, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.000435851514339447
Iteration: 1352, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004351344425231218
Iteration: 1353, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002218943554908037
Iteration: 1354, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0043965051881968975
Iteration: 1355, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004071502014994621
Iteration: 1356, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0027894191443920135
Iteration: 1357, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005071098916232586
Iteration: 1358, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006182386539876461
Collision detected, score: 0
Iteration: 1359, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.004826070740818977
Iteration: 1360, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006759208161383867
Iteration: 1361, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006693566218018532
Iteration: 1362, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002938333433121443
Iteration: 1363, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007441177498549223
Iteration: 1364, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1365, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1366, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1367, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1368, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1369, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1370, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1371, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1372, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1373, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1374, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1375, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1376, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1377, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1378, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1379, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1380, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1381, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1382, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019325539469718933
Iteration: 1383, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019290298223495483
Iteration: 1384, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000776718370616436
Iteration: 1385, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010877735912799835
Iteration: 1386, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00015596207231283188
Perfoming an random action
Iteration: 1387, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0016050133854150772
Iteration: 1388, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00014628469944000244
Iteration: 1389, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001481102779507637
Perfoming an random action
Iteration: 1390, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033750534057617188
Iteration: 1391, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009039155207574368
Iteration: 1392, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009107258170843124
Iteration: 1393, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005275954492390156
Iteration: 1394, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007993172854185104
Iteration: 1395, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00944402627646923
Iteration: 1396, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00919868890196085
Iteration: 1397, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01313027273863554
Iteration: 1398, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007075444329530001
Iteration: 1399, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011719765141606331
Iteration: 1400, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012480729259550571
Iteration: 1401, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011020946316421032
Iteration: 1402, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011301746591925621
Iteration: 1403, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006783375516533852
Iteration: 1404, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00970444455742836
Iteration: 1405, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007426725700497627
Iteration: 1406, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0067305550910532475
Iteration: 1407, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006837638095021248
Iteration: 1408, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005576419644057751
Iteration: 1409, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008686739951372147
Iteration: 1410, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0045165251940488815
Iteration: 1411, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004590865224599838
Iteration: 1412, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00013527553528547287
Iteration: 1413, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0036064190790057182
Iteration: 1414, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005115624517202377
Iteration: 1415, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007695462554693222
Iteration: 1416, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0016201864928007126
Collision detected, score: 0
Iteration: 1417, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0004715118557214737
Iteration: 1418, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017412956804037094
Iteration: 1419, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00724320113658905
Iteration: 1420, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003086823970079422
Iteration: 1421, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016136467456817627
Iteration: 1422, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 1423, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 1424, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 1425, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 1426, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 1427, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 1428, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 1429, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 1430, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 1431, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 1432, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 1433, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 1434, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 1435, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 1436, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 1437, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 1438, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 1439, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 1440, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000754169188439846
Iteration: 1441, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002286229282617569
Iteration: 1442, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027727019041776657
Collision detected, score: 0
Iteration: 1443, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0015059895813465118
Iteration: 1444, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00010044686496257782
Iteration: 1445, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007473294623196125
Iteration: 1446, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0052845776081085205
Iteration: 1447, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006010419689118862
Iteration: 1448, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1449, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1450, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1451, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1452, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1453, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1454, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1455, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1456, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1457, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1458, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1459, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1460, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1461, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1462, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1463, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1464, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1465, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1466, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019658980891108513
Iteration: 1467, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001266644336283207
Perfoming an random action
Iteration: 1468, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013643810525536537
Iteration: 1469, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007722973823547363
Iteration: 1470, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020904541015625
Iteration: 1471, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002644490450620651
Iteration: 1472, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018999837338924408
Iteration: 1473, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007856320589780807
Iteration: 1474, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004175205249339342
Iteration: 1475, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007704992778599262
Iteration: 1476, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011597054079174995
Iteration: 1477, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007757348008453846
Iteration: 1478, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007876083254814148
Iteration: 1479, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009970622137188911
Iteration: 1480, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005290346220135689
Iteration: 1481, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007615802809596062
Iteration: 1482, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0079846465960145
Iteration: 1483, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0072912597097456455
Iteration: 1484, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009385979734361172
Iteration: 1485, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008134855888783932
Iteration: 1486, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009507696144282818
Iteration: 1487, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007325843442231417
Iteration: 1488, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012310178950428963
Iteration: 1489, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004947701469063759
Iteration: 1490, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008237100206315517
Iteration: 1491, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007224534638226032
Iteration: 1492, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008084304630756378
Iteration: 1493, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004106464795768261
Iteration: 1494, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006231768056750298
Iteration: 1495, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005775038152933121
Iteration: 1496, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005827806890010834
Iteration: 1497, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0016741259023547173
Iteration: 1498, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0033264942467212677
Iteration: 1499, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004082293715327978
Iteration: 1500, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002933766692876816
Iteration: 1501, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005084684584289789
Iteration: 1502, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007055535912513733
Iteration: 1503, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005116241984069347
Iteration: 1504, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019554132595658302
Iteration: 1505, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0016552042216062546
Collision detected, score: 0
Iteration: 1506, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.0050320993177592754
Iteration: 1507, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036035203374922276
Iteration: 1508, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007113673258572817
Iteration: 1509, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013075927272439003
Iteration: 1510, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032844943925738335
Iteration: 1511, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 1512, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017140824347734451
Iteration: 1513, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016389107331633568
Iteration: 1514, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014947187155485153
Iteration: 1515, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001966129057109356
Iteration: 1516, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004209826700389385
Iteration: 1517, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004745318088680506
Iteration: 1518, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003355017863214016
Iteration: 1519, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004265457391738892
Iteration: 1520, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004399097058922052
Iteration: 1521, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004322832450270653
Iteration: 1522, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019652200862765312
Iteration: 1523, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001681785099208355
Iteration: 1524, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006626388058066368
Iteration: 1525, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003104171250015497
Iteration: 1526, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034972475841641426
Iteration: 1527, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008184014819562435
Iteration: 1528, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00528718251734972
Iteration: 1529, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00475411769002676
Iteration: 1530, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003255699761211872
Iteration: 1531, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030572558753192425
Iteration: 1532, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0052795433439314365
Iteration: 1533, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009474596008658409
Iteration: 1534, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0042522926814854145
Iteration: 1535, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003955739550292492
Iteration: 1536, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026397332549095154
Iteration: 1537, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00037240050733089447
Iteration: 1538, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003256415482610464
Iteration: 1539, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004525662865489721
Iteration: 1540, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0050742533057928085
Iteration: 1541, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005936540197581053
Iteration: 1542, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006260358728468418
Iteration: 1543, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004752823151648045
Iteration: 1544, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0050622145645320415
Iteration: 1545, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005427158437669277
Iteration: 1546, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034057991579174995
Iteration: 1547, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00955653004348278
Iteration: 1548, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006429056636989117
Iteration: 1549, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033687539398670197
Iteration: 1550, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006823570933192968
Iteration: 1551, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005501994863152504
Iteration: 1552, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001772427000105381
Iteration: 1553, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0007568970322608948
Iteration: 1554, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012115081772208214
Iteration: 1555, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0019358526915311813
Iteration: 1556, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014382535591721535
Iteration: 1557, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00044303759932518005
Collision detected, score: 0
Iteration: 1558, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0004927432164549828
Iteration: 1559, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031039738096296787
Iteration: 1560, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01368649210780859
Iteration: 1561, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009511704556643963
Iteration: 1562, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005666919052600861
Iteration: 1563, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1564, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1565, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1566, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1567, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1568, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1569, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1570, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1571, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1572, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1573, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1574, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1575, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1576, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1577, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1578, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1579, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1580, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1581, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019325539469718933
Iteration: 1582, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018689408898353577
Iteration: 1583, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000392683781683445
Iteration: 1584, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009867511689662933
Iteration: 1585, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00024676136672496796
Iteration: 1586, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014737341552972794
Iteration: 1587, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00027188193053007126
Iteration: 1588, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016528423875570297
Iteration: 1589, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003403372596949339
Iteration: 1590, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008946957066655159
Iteration: 1591, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009795548394322395
Iteration: 1592, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003488861955702305
Iteration: 1593, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01009045448154211
Iteration: 1594, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011267293244600296
Iteration: 1595, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007978376001119614
Iteration: 1596, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010275415144860744
Iteration: 1597, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010018786415457726
Iteration: 1598, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012146782130002975
Iteration: 1599, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008208361454308033
Iteration: 1600, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003978211898356676
Iteration: 1601, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009948277845978737
Iteration: 1602, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009610642679035664
Iteration: 1603, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009184683673083782
Iteration: 1604, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007716561201959848
Iteration: 1605, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003288994077593088
Iteration: 1606, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034093675203621387
Iteration: 1607, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029381606727838516
Iteration: 1608, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002938622608780861
Iteration: 1609, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002741629257798195
Iteration: 1610, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0013459306210279465
Iteration: 1611, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0024761175736784935
Iteration: 1612, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006401566788554192
Iteration: 1613, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001392081379890442
Iteration: 1614, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0001299697905778885
Iteration: 1615, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012780101969838142
Iteration: 1616, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020976662635803223
Iteration: 1617, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011132648214697838
Iteration: 1618, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 1.9244849681854248e-05
Iteration: 1619, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0015917029231786728
Iteration: 1620, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002137487754225731
Iteration: 1621, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018669385462999344
Iteration: 1622, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004355780314654112
Iteration: 1623, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038539301604032516
Iteration: 1624, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005230688489973545
Iteration: 1625, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026411758735775948
Iteration: 1626, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00540588516741991
Collision detected, score: 0
Iteration: 1627, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.006643299013376236
Iteration: 1628, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004469604231417179
Iteration: 1629, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00515485554933548
Iteration: 1630, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006589720956981182
Iteration: 1631, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004203356336802244
Iteration: 1632, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1633, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1634, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1635, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1636, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1637, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1638, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1639, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1640, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1641, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1642, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1643, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1644, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1645, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1646, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1647, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1648, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1649, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1650, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021433578804135323
Iteration: 1651, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013946117833256721
Iteration: 1652, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00046442635357379913
Iteration: 1653, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00019229762256145477
Iteration: 1654, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000998486764729023
Iteration: 1655, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00011768564581871033
Iteration: 1656, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018869312480092049
Iteration: 1657, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023842263035476208
Iteration: 1658, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004185202065855265
Iteration: 1659, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005846191197633743
Iteration: 1660, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006661355495452881
Iteration: 1661, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018952218815684319
Iteration: 1662, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015837792307138443
Iteration: 1663, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029016011394560337
Iteration: 1664, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0027861730195581913
Iteration: 1665, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006168515887111425
Iteration: 1666, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028221290558576584
Iteration: 1667, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004083104431629181
Iteration: 1668, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016023265197873116
Iteration: 1669, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003233008086681366
Iteration: 1670, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001423550769686699
Iteration: 1671, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004058279097080231
Iteration: 1672, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006229376420378685
Iteration: 1673, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004672951530665159
Iteration: 1674, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029906919226050377
Iteration: 1675, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009749764576554298
Iteration: 1676, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00840467307716608
Iteration: 1677, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007737794890999794
Iteration: 1678, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006832391954958439
Iteration: 1679, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00682283379137516
Iteration: 1680, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00281778397038579
Iteration: 1681, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001076837070286274
Iteration: 1682, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025521712377667427
Iteration: 1683, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034967083483934402
Iteration: 1684, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009199881926178932
Iteration: 1685, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005829472094774246
Iteration: 1686, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019956594333052635
Iteration: 1687, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005635661538690329
Iteration: 1688, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001520722173154354
Iteration: 1689, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002092696726322174
Iteration: 1690, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004410766065120697
Iteration: 1691, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008167681284248829
Iteration: 1692, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009892440401017666
Iteration: 1693, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009434790350496769
Iteration: 1694, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008751744404435158
Iteration: 1695, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039723943918943405
Iteration: 1696, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002269214950501919
Iteration: 1697, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018007606267929077
Iteration: 1698, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005148618947714567
Iteration: 1699, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001996612176299095
Iteration: 1700, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00039641931653022766
Iteration: 1701, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007557561621069908
Collision detected, score: 0
Iteration: 1702, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00019467249512672424
Iteration: 1703, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003277433104813099
Iteration: 1704, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007676476612687111
Iteration: 1705, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007732606492936611
Iteration: 1706, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004696724936366081
Iteration: 1707, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1708, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1709, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1710, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1711, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1712, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1713, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1714, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1715, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1716, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1717, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1718, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1719, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1720, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1721, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1722, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1723, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1724, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1725, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015140064060688019
Iteration: 1726, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016413470730185509
Iteration: 1727, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00016375910490751266
Iteration: 1728, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005159592255949974
Iteration: 1729, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013309000059962273
Iteration: 1730, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009123384952545166
Iteration: 1731, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004097628872841597
Iteration: 1732, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00452720932662487
Iteration: 1733, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005305328872054815
Iteration: 1734, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0070917317643761635
Iteration: 1735, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007841592654585838
Iteration: 1736, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005020147655159235
Iteration: 1737, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034493193961679935
Iteration: 1738, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0044523123651742935
Iteration: 1739, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024407990276813507
Iteration: 1740, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002185746096074581
Iteration: 1741, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004651445429772139
Iteration: 1742, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015595797449350357
Iteration: 1743, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023786951787769794
Iteration: 1744, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034066508524119854
Iteration: 1745, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037882905453443527
Iteration: 1746, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.010104160755872726
Iteration: 1747, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006478109862655401
Iteration: 1748, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007229964714497328
Iteration: 1749, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0046358914114534855
Iteration: 1750, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004664027132093906
Iteration: 1751, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006536649074405432
Iteration: 1752, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0070470222271978855
Iteration: 1753, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005893099121749401
Iteration: 1754, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.569681525230408e-05
Iteration: 1755, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00295151025056839
Iteration: 1756, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004619477316737175
Iteration: 1757, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004700423683971167
Iteration: 1758, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033914968371391296
Iteration: 1759, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040477910079061985
Iteration: 1760, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006346173584461212
Iteration: 1761, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003313834313303232
Iteration: 1762, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036049913614988327
Iteration: 1763, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009718481451272964
Collision detected, score: 0
Iteration: 1764, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0033413777127861977
Iteration: 1765, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010911189019680023
Iteration: 1766, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01188621111214161
Iteration: 1767, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010662103071808815
Iteration: 1768, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00433009397238493
Iteration: 1769, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1770, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1771, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1772, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1773, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1774, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1775, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1776, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1777, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1778, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1779, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1780, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1781, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1782, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1783, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1784, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1785, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1786, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1787, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015427414327859879
Iteration: 1788, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001735970377922058
Iteration: 1789, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 8.628703653812408e-05
Iteration: 1790, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0002378411591053009
Iteration: 1791, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00029154960066080093
Iteration: 1792, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0016816901043057442
Iteration: 1793, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005297008901834488
Iteration: 1794, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024747964926064014
Iteration: 1795, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017543602734804153
Iteration: 1796, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008675499819219112
Iteration: 1797, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007159811444580555
Iteration: 1798, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005715731531381607
Iteration: 1799, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007310300134122372
Iteration: 1800, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007017691619694233
Iteration: 1801, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00868300348520279
Iteration: 1802, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005249421112239361
Iteration: 1803, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00846080295741558
Iteration: 1804, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01056484691798687
Iteration: 1805, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011756847612559795
Iteration: 1806, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011674665845930576
Iteration: 1807, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013754111714661121
Iteration: 1808, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008380166254937649
Iteration: 1809, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010084833018481731
Iteration: 1810, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01048902329057455
Iteration: 1811, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009142451919615269
Iteration: 1812, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0099418256431818
Iteration: 1813, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007408373989164829
Iteration: 1814, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009791133925318718
Iteration: 1815, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00642463518306613
Iteration: 1816, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002849468495696783
Iteration: 1817, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034834938123822212
Iteration: 1818, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0032004956156015396
Iteration: 1819, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00015794485807418823
Iteration: 1820, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0012008026242256165
Iteration: 1821, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011310838162899017
Iteration: 1822, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016526924446225166
Iteration: 1823, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034704096615314484
Iteration: 1824, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0055787935853004456
Iteration: 1825, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0018498776480555534
Iteration: 1826, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0035811131820082664
Iteration: 1827, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003428107127547264
Iteration: 1828, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036467681638896465
Iteration: 1829, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005346912890672684
Iteration: 1830, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003055005334317684
Iteration: 1831, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015088627114892006
Iteration: 1832, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002277061343193054
Iteration: 1833, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00848361849784851
Iteration: 1834, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0020344341173768044
Iteration: 1835, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036529777571558952
Collision detected, score: 0
Iteration: 1836, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.006231746636331081
Iteration: 1837, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012318877503275871
Iteration: 1838, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006546821445226669
Iteration: 1839, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006707481574267149
Iteration: 1840, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00807788223028183
Iteration: 1841, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1842, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1843, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1844, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1845, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1846, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1847, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1848, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1849, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1850, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1851, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1852, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1853, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1854, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1855, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1856, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1857, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1858, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1859, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017989454790949821
Iteration: 1860, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020709186792373657
Iteration: 1861, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006758170202374458
Iteration: 1862, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012317728251218796
Iteration: 1863, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -8.291564881801605e-05
Iteration: 1864, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014054849743843079
Iteration: 1865, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0010352404788136482
Iteration: 1866, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017376774922013283
Iteration: 1867, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003178328275680542
Iteration: 1868, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00873425230383873
Iteration: 1869, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009863460436463356
Iteration: 1870, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004523326642811298
Iteration: 1871, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00838408526033163
Iteration: 1872, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010792848654091358
Iteration: 1873, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007534401956945658
Iteration: 1874, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01008842047303915
Iteration: 1875, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010523010045289993
Iteration: 1876, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012685580179095268
Iteration: 1877, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0077387066558003426
Iteration: 1878, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006582177709788084
Iteration: 1879, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010618605650961399
Iteration: 1880, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008144729770720005
Iteration: 1881, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006577366963028908
Iteration: 1882, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007565555162727833
Iteration: 1883, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004372606985270977
Iteration: 1884, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039053563959896564
Iteration: 1885, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005086992401629686
Iteration: 1886, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002738657407462597
Iteration: 1887, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028090686537325382
Iteration: 1888, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000562349334359169
Iteration: 1889, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002050774171948433
Iteration: 1890, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006405450403690338
Iteration: 1891, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001118263229727745
Iteration: 1892, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00021594297140836716
Iteration: 1893, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011657830327749252
Iteration: 1894, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002127675339579582
Iteration: 1895, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017816899344325066
Iteration: 1896, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019431263208389282
Iteration: 1897, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0021154414862394333
Iteration: 1898, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0018828082829713821
Iteration: 1899, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025083296932280064
Iteration: 1900, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004246523138135672
Iteration: 1901, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005533798597753048
Iteration: 1902, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007084779907017946
Iteration: 1903, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029272637329995632
Iteration: 1904, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005030396394431591
Collision detected, score: 0
Iteration: 1905, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.005527735687792301
Iteration: 1906, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004590381868183613
Iteration: 1907, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004736721981316805
Iteration: 1908, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006589720956981182
Iteration: 1909, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004203356336802244
Iteration: 1910, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 1911, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 1912, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 1913, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 1914, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 1915, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 1916, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 1917, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 1918, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 1919, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 1920, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 1921, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 1922, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 1923, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 1924, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 1925, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 1926, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 1927, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 1928, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022746492177248
Iteration: 1929, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011529652401804924
Iteration: 1930, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012230779975652695
Iteration: 1931, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00030445028096437454
Iteration: 1932, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000651421956717968
Iteration: 1933, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 2.2542662918567657e-05
Iteration: 1934, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003266233019530773
Iteration: 1935, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0045687127858400345
Iteration: 1936, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00420076260343194
Iteration: 1937, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009937542490661144
Iteration: 1938, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011340656317770481
Iteration: 1939, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0069394782185554504
Iteration: 1940, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01092433463782072
Iteration: 1941, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005505440291017294
Iteration: 1942, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005430638790130615
Iteration: 1943, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005097271874547005
Iteration: 1944, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005673485808074474
Iteration: 1945, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00956428050994873
Iteration: 1946, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004622169770300388
Iteration: 1947, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006662571337074041
Iteration: 1948, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005547982174903154
Iteration: 1949, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00793695729225874
Iteration: 1950, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0049386885948479176
Iteration: 1951, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00492747500538826
Iteration: 1952, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005680774338543415
Iteration: 1953, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032330891117453575
Iteration: 1954, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0057009318843483925
Iteration: 1955, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004040384199470282
Iteration: 1956, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003812551498413086
Iteration: 1957, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000116017647087574
Iteration: 1958, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0015173573046922684
Iteration: 1959, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014228308573365211
Iteration: 1960, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009759552776813507
Iteration: 1961, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0052460916340351105
Iteration: 1962, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006212179083377123
Iteration: 1963, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00339875603094697
Iteration: 1964, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009863286279141903
Iteration: 1965, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00479747261852026
Iteration: 1966, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.003371431492269039
Iteration: 1967, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0031941719353199005
Collision detected, score: 0
Iteration: 1968, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.0035266815684735775
Iteration: 1969, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005269404035061598
Iteration: 1970, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00012519210577011108
Iteration: 1971, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027660150080919266
Iteration: 1972, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037345699965953827
Iteration: 1973, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 1974, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 1975, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 1976, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 1977, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 1978, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 1979, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 1980, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 1981, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 1982, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 1983, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 1984, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 1985, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 1986, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 1987, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 1988, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 1989, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 1990, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 1991, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008418858051300049
Iteration: 1992, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024840696714818478
Iteration: 1993, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031433552503585815
Collision detected, score: 0
Iteration: 1994, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0023363279178738594
Iteration: 1995, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010524243116378784
Iteration: 1996, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006256118416786194
Iteration: 1997, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005111084785312414
Iteration: 1998, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00027781538665294647
Iteration: 1999, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 2000, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 2001, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 2002, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 2003, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 2004, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 2005, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 2006, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 2007, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 2008, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 2009, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 2010, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 2011, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 2012, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 2013, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 2014, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 2015, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 2016, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 2017, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006223929114639759
Iteration: 2018, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037941448390483856
Iteration: 2019, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003995665814727545
Iteration: 2020, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003543481230735779
Iteration: 2021, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047528501600027084
Iteration: 2022, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008462346158921719
Iteration: 2023, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002248817589133978
Iteration: 2024, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.008147032931447029
Iteration: 2025, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008383220061659813
Iteration: 2026, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008927400223910809
Iteration: 2027, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009793323464691639
Iteration: 2028, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006027414463460445
Iteration: 2029, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003978806547820568
Iteration: 2030, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009268456138670444
Iteration: 2031, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007827659137547016
Iteration: 2032, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00878157652914524
Iteration: 2033, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00814556609839201
Iteration: 2034, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00866625364869833
Iteration: 2035, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007313775829970837
Iteration: 2036, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005023892503231764
Perfoming an random action
Iteration: 2037, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047079529613256454
Iteration: 2038, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006142646074295044
Iteration: 2039, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004667781759053469
Iteration: 2040, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008551728911697865
Iteration: 2041, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002033715136349201
Iteration: 2042, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006811097264289856
Iteration: 2043, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036996048875153065
Iteration: 2044, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003751471173018217
Iteration: 2045, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005734770558774471
Iteration: 2046, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004007446579635143
Iteration: 2047, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012333262711763382
Iteration: 2048, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0004656892269849777
Iteration: 2049, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035396479070186615
Iteration: 2050, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018000723794102669
Iteration: 2051, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004004587419331074
Iteration: 2052, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032294876873493195
Iteration: 2053, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002145431935787201
Iteration: 2054, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012463126331567764
Iteration: 2055, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0023440467193722725
Iteration: 2056, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.006423436105251312
Collision detected, score: 0
Iteration: 2057, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: -0.0017832517623901367
Iteration: 2058, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0004277229309082031
Iteration: 2059, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002773885615170002
Iteration: 2060, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005230666138231754
Iteration: 2061, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021402016282081604
Iteration: 2062, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 2063, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 2064, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 2065, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 2066, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 2067, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 2068, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 2069, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 2070, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 2071, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 2072, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 2073, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 2074, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 2075, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 2076, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 2077, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 2078, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 2079, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 2080, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004984401166439056
Iteration: 2081, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002707516774535179
Iteration: 2082, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034235804341733456
Collision detected, score: 0
Iteration: 2083, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002154175192117691
Iteration: 2084, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008526518940925598
Iteration: 2085, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007579410448670387
Iteration: 2086, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006892362143844366
Iteration: 2087, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005924615077674389
Iteration: 2088, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2089, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2090, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2091, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2092, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Perfoming an random action
Iteration: 2093, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2094, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2095, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036104791797697544
Iteration: 2096, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001965099945664406
Iteration: 2097, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002469649538397789
Iteration: 2098, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026532523334026337
Iteration: 2099, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005270417779684067
Iteration: 2100, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004246781580150127
Iteration: 2101, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00496479868888855
Iteration: 2102, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021903016604483128
Iteration: 2103, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002758763264864683
Iteration: 2104, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002673849929124117
Iteration: 2105, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00027129240334033966
Iteration: 2106, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003173581324517727
Iteration: 2107, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013603679835796356
Iteration: 2108, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001764785498380661
Iteration: 2109, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033881794661283493
Iteration: 2110, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023879120126366615
Iteration: 2111, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002883328590542078
Iteration: 2112, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003204044420272112
Iteration: 2113, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036811111494898796
Iteration: 2114, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011826500296592712
Iteration: 2115, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00878178235143423
Iteration: 2116, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009052477777004242
Iteration: 2117, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007906981743872166
Iteration: 2118, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043888031505048275
Iteration: 2119, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0044303955510258675
Iteration: 2120, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005575916729867458
Iteration: 2121, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0051931776106357574
Iteration: 2122, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007766668684780598
Iteration: 2123, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00899631530046463
Iteration: 2124, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008581317961215973
Iteration: 2125, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009353709407150745
Iteration: 2126, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007079877890646458
Iteration: 2127, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010152682662010193
Iteration: 2128, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00790087878704071
Iteration: 2129, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00596508476883173
Iteration: 2130, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038418108597397804
Iteration: 2131, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008365957997739315
Iteration: 2132, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008417041972279549
Iteration: 2133, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011604536324739456
Iteration: 2134, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003703361377120018
Iteration: 2135, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024080341681838036
Iteration: 2136, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003110760822892189
Iteration: 2137, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00010366365313529968
Iteration: 2138, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0013126973062753677
Iteration: 2139, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.003334193490445614
Iteration: 2140, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016820626333355904
Iteration: 2141, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010683899745345116
Iteration: 2142, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00011911150068044662
Iteration: 2143, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026682578027248383
Iteration: 2144, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0015807924792170525
Iteration: 2145, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002617835998535156
Collision detected, score: 0
Iteration: 2146, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.001961718313395977
Iteration: 2147, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016552917659282684
Iteration: 2148, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004058958031237125
Iteration: 2149, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006224577780812979
Iteration: 2150, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006336337886750698
Iteration: 2151, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2152, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2153, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2154, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2155, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2156, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2157, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2158, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2159, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2160, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2161, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2162, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2163, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2164, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2165, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2166, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2167, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2168, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2169, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016931947320699692
Iteration: 2170, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015662703663110733
Iteration: 2171, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -3.522820770740509e-05
Iteration: 2172, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0002622324973344803
Iteration: 2173, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014208219945430756
Iteration: 2174, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005580699071288109
Iteration: 2175, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037107919342815876
Iteration: 2176, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00434197997674346
Iteration: 2177, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034878128208220005
Iteration: 2178, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005169300362467766
Iteration: 2179, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007312939502298832
Iteration: 2180, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004077920224517584
Iteration: 2181, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002829001285135746
Iteration: 2182, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004633762873709202
Iteration: 2183, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025475090369582176
Iteration: 2184, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005312357563525438
Iteration: 2185, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005747936200350523
Iteration: 2186, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00438543688505888
Iteration: 2187, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005477480590343475
Iteration: 2188, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011593177914619446
Iteration: 2189, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017678570002317429
Iteration: 2190, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006119351834058762
Iteration: 2191, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0044020116329193115
Iteration: 2192, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005614588502794504
Iteration: 2193, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016937637701630592
Iteration: 2194, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009080599993467331
Iteration: 2195, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004900558851659298
Iteration: 2196, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005173837766051292
Iteration: 2197, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042615365236997604
Iteration: 2198, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010855086147785187
Iteration: 2199, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003638879396021366
Iteration: 2200, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023335590958595276
Iteration: 2201, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005450189113616943
Iteration: 2202, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005070103332400322
Iteration: 2203, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003803955391049385
Iteration: 2204, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0065361494198441505
Iteration: 2205, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021722782403230667
Iteration: 2206, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003357900306582451
Iteration: 2207, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042033265344798565
Iteration: 2208, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002784792333841324
Collision detected, score: 0
Iteration: 2209, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002453446388244629
Iteration: 2210, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034285513684153557
Iteration: 2211, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013236220926046371
Iteration: 2212, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00869995541870594
Iteration: 2213, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018291231244802475
Iteration: 2214, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 2215, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 2216, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 2217, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 2218, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 2219, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 2220, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 2221, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 2222, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 2223, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 2224, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 2225, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 2226, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 2227, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 2228, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 2229, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 2230, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 2231, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 2232, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005884412676095963
Iteration: 2233, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023491191677749157
Iteration: 2234, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029812282882630825
Collision detected, score: 0
Iteration: 2235, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002154870890080929
Iteration: 2236, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005117543041706085
Iteration: 2237, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005420595873147249
Iteration: 2238, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004688214976340532
Iteration: 2239, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012362999841570854
Iteration: 2240, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 2241, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 2242, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 2243, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 2244, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 2245, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 2246, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 2247, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 2248, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 2249, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 2250, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 2251, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 2252, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 2253, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 2254, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 2255, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 2256, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 2257, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 2258, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005804502405226231
Iteration: 2259, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003774813376367092
Iteration: 2260, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004108848981559277
Iteration: 2261, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024624913930892944
Iteration: 2262, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028841253370046616
Iteration: 2263, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007138781249523163
Iteration: 2264, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036374060437083244
Iteration: 2265, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009578919969499111
Iteration: 2266, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0066316993907094
Iteration: 2267, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007967796176671982
Iteration: 2268, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007443910930305719
Iteration: 2269, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008601645939052105
Iteration: 2270, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007366820704191923
Iteration: 2271, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009771774522960186
Iteration: 2272, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008058999665081501
Iteration: 2273, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0052668009884655476
Iteration: 2274, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007748832460492849
Iteration: 2275, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004993969574570656
Iteration: 2276, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011236276477575302
Iteration: 2277, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010551484301686287
Iteration: 2278, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004724777769297361
Iteration: 2279, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003844653256237507
Iteration: 2280, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005245794542133808
Iteration: 2281, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035876543261110783
Iteration: 2282, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00210532546043396
Iteration: 2283, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001684781163930893
Iteration: 2284, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028537455946207047
Iteration: 2285, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007072119042277336
Iteration: 2286, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007993187755346298
Iteration: 2287, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002965581603348255
Iteration: 2288, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 1.9985251128673553e-05
Iteration: 2289, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007688729092478752
Iteration: 2290, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029869298450648785
Collision detected, score: 0
Iteration: 2291, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.006615025922656059
Iteration: 2292, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00438313465565443
Iteration: 2293, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009634280577301979
Iteration: 2294, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005598162766546011
Iteration: 2295, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022285040467977524
Iteration: 2296, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2297, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2298, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2299, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2300, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2301, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2302, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2303, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2304, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2305, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2306, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2307, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2308, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2309, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2310, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2311, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Perfoming an random action
Iteration: 2312, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2313, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2314, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001854725182056427
Iteration: 2315, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012923413887619972
Iteration: 2316, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0003313552588224411
Iteration: 2317, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009012306109070778
Iteration: 2318, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005881059914827347
Iteration: 2319, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0001964513212442398
Iteration: 2320, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007806364446878433
Iteration: 2321, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005012159701436758
Iteration: 2322, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003032477106899023
Iteration: 2323, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010724179446697235
Iteration: 2324, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009958889335393906
Iteration: 2325, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007750757969915867
Iteration: 2326, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009858710691332817
Iteration: 2327, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008928051218390465
Iteration: 2328, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009527658112347126
Iteration: 2329, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010908935219049454
Iteration: 2330, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008253070525825024
Iteration: 2331, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012256225571036339
Iteration: 2332, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006461649667471647
Iteration: 2333, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0064406804740428925
Iteration: 2334, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0051969075575470924
Iteration: 2335, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005096075590699911
Iteration: 2336, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0063420673832297325
Iteration: 2337, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005432299338281155
Iteration: 2338, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004364705178886652
Iteration: 2339, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026100780814886093
Iteration: 2340, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014284048229455948
Iteration: 2341, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006103413179516792
Iteration: 2342, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005565891042351723
Iteration: 2343, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00206640362739563
Iteration: 2344, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003540744073688984
Iteration: 2345, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003727128729224205
Iteration: 2346, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004667630884796381
Iteration: 2347, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0055256374180316925
Iteration: 2348, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006672278977930546
Iteration: 2349, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004250222351402044
Iteration: 2350, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002100624144077301
Iteration: 2351, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018867934122681618
Iteration: 2352, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002895939163863659
Iteration: 2353, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -7.042009383440018e-05
Iteration: 2354, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005830628797411919
Collision detected, score: 0
Iteration: 2355, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: -0.0012197140604257584
Iteration: 2356, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008795706555247307
Iteration: 2357, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028606639243662357
Iteration: 2358, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004361242987215519
Iteration: 2359, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004047668073326349
Iteration: 2360, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2361, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2362, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2363, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2364, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2365, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2366, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2367, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2368, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2369, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2370, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2371, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2372, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2373, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2374, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2375, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2376, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2377, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2378, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002003614790737629
Iteration: 2379, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001372823491692543
Iteration: 2380, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015477398410439491
Iteration: 2381, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0002052895724773407
Iteration: 2382, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00015712808817625046
Iteration: 2383, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0008134962990880013
Iteration: 2384, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002408865839242935
Iteration: 2385, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003179779276251793
Iteration: 2386, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004041219595819712
Iteration: 2387, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007908690720796585
Iteration: 2388, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006473308429121971
Iteration: 2389, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004477840848267078
Iteration: 2390, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006439171731472015
Iteration: 2391, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004513539839535952
Iteration: 2392, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004341695923358202
Iteration: 2393, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011257175356149673
Iteration: 2394, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036637168377637863
Iteration: 2395, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003532322123646736
Iteration: 2396, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00775560550391674
Iteration: 2397, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007278938312083483
Iteration: 2398, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006842153146862984
Iteration: 2399, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005635993555188179
Iteration: 2400, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009565378539264202
Iteration: 2401, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007528855465352535
Iteration: 2402, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007454659789800644
Iteration: 2403, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003333238884806633
Iteration: 2404, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010467632673680782
Iteration: 2405, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009248558431863785
Iteration: 2406, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008501682430505753
Iteration: 2407, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009206370450556278
Iteration: 2408, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004225082695484161
Iteration: 2409, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006973092444241047
Iteration: 2410, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006373733282089233
Iteration: 2411, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027724946849048138
Iteration: 2412, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009447850286960602
Iteration: 2413, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005741419270634651
Iteration: 2414, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004135210998356342
Iteration: 2415, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003603191114962101
Iteration: 2416, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0055173784494400024
Perfoming an random action
Iteration: 2417, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002628023736178875
Collision detected, score: 0
Iteration: 2418, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.00585009204223752
Iteration: 2419, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006159531883895397
Iteration: 2420, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005926528479903936
Iteration: 2421, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032911659218370914
Iteration: 2422, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002053428441286087
Iteration: 2423, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 2424, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017140824347734451
Iteration: 2425, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016389107331633568
Iteration: 2426, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014947187155485153
Iteration: 2427, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001966129057109356
Iteration: 2428, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004209826700389385
Iteration: 2429, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004745318088680506
Iteration: 2430, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003355017863214016
Iteration: 2431, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004265457391738892
Iteration: 2432, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004399097058922052
Iteration: 2433, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004322832450270653
Iteration: 2434, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019652200862765312
Iteration: 2435, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001681785099208355
Iteration: 2436, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006626388058066368
Iteration: 2437, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003104171250015497
Iteration: 2438, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034972475841641426
Iteration: 2439, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008184014819562435
Iteration: 2440, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00528718251734972
Iteration: 2441, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004821217618882656
Iteration: 2442, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003910408355295658
Iteration: 2443, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032051987946033478
Iteration: 2444, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00566419493407011
Iteration: 2445, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0002362765371799469
Iteration: 2446, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032430323772132397
Iteration: 2447, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005033429712057114
Iteration: 2448, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004534536506980658
Iteration: 2449, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006663457490503788
Iteration: 2450, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00845189206302166
Iteration: 2451, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00887870043516159
Iteration: 2452, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005811604671180248
Iteration: 2453, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007161520887166262
Iteration: 2454, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009362808428704739
Iteration: 2455, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005744277499616146
Iteration: 2456, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003493133932352066
Iteration: 2457, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005956100765615702
Iteration: 2458, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029210057109594345
Iteration: 2459, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025452757254242897
Iteration: 2460, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006964872591197491
Iteration: 2461, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007092765532433987
Iteration: 2462, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006594070233404636
Iteration: 2463, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035833753645420074
Iteration: 2464, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010231006890535355
Iteration: 2465, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033823885023593903
Iteration: 2466, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016876636072993279
Iteration: 2467, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001601124182343483
Iteration: 2468, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026992950588464737
Iteration: 2469, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028337519615888596
Iteration: 2470, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00450405478477478
Iteration: 2471, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -7.885880768299103e-05
Iteration: 2472, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004660121165215969
Iteration: 2473, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0055994740687310696
Iteration: 2474, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005456077866256237
Iteration: 2475, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004064700100570917
Iteration: 2476, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000596245750784874
Iteration: 2477, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003889314830303192
Iteration: 2478, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015507787466049194
Iteration: 2479, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043800584971904755
Iteration: 2480, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037598530761897564
Iteration: 2481, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038996203802525997
Iteration: 2482, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004847933072596788
Collision detected, score: 0
Iteration: 2483, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.003436553291976452
Iteration: 2484, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005665269680321217
Iteration: 2485, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005326212849467993
Iteration: 2486, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006384884938597679
Iteration: 2487, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004936848767101765
Iteration: 2488, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2489, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2490, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2491, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2492, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2493, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2494, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2495, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2496, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2497, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2498, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2499, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2500, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2501, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2502, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2503, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2504, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2505, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2506, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001855095848441124
Iteration: 2507, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008703162893652916
Iteration: 2508, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007324088364839554
Iteration: 2509, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0003294721245765686
Iteration: 2510, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016418518498539925
Iteration: 2511, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0001765163615345955
Iteration: 2512, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00123558659106493
Iteration: 2513, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011915769428014755
Iteration: 2514, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004110964480787516
Iteration: 2515, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00825628824532032
Iteration: 2516, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01233093161135912
Iteration: 2517, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009528003633022308
Iteration: 2518, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006690358277410269
Iteration: 2519, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006955601274967194
Iteration: 2520, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0059515549801290035
Iteration: 2521, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005842633545398712
Iteration: 2522, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007163584232330322
Iteration: 2523, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008258715271949768
Iteration: 2524, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005676775239408016
Iteration: 2525, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005456195678561926
Iteration: 2526, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0069735050201416016
Iteration: 2527, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013546476140618324
Iteration: 2528, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009932865388691425
Iteration: 2529, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007191129494458437
Iteration: 2530, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007783827837556601
Iteration: 2531, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0062309871427714825
Iteration: 2532, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007967747747898102
Perfoming an random action
Iteration: 2533, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010061324574053288
Iteration: 2534, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042844489216804504
Iteration: 2535, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005120549350976944
Iteration: 2536, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005961003713309765
Iteration: 2537, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008100578561425209
Iteration: 2538, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007676510140299797
Iteration: 2539, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004963850602507591
Iteration: 2540, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006594848819077015
Iteration: 2541, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009203445166349411
Iteration: 2542, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00829670112580061
Iteration: 2543, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019667986780405045
Iteration: 2544, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005002610385417938
Iteration: 2545, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008343172259628773
Iteration: 2546, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008186898194253445
Iteration: 2547, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006668042857199907
Iteration: 2548, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007579971570521593
Collision detected, score: 0
Iteration: 2549, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0028437948785722256
Iteration: 2550, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004545819014310837
Iteration: 2551, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007458891719579697
Iteration: 2552, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0060257562436163425
Iteration: 2553, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010217288509011269
Iteration: 2554, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 2555, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009051002562046051
Iteration: 2556, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00021384935826063156
Iteration: 2557, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012660948559641838
Iteration: 2558, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018334342166781425
Iteration: 2559, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0044052572920918465
Iteration: 2560, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005087495781481266
Iteration: 2561, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028295740485191345
Iteration: 2562, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036235665902495384
Iteration: 2563, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011557694524526596
Iteration: 2564, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002689965069293976
Iteration: 2565, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00020991358906030655
Iteration: 2566, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00014249607920646667
Iteration: 2567, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002909666858613491
Iteration: 2568, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013370774686336517
Iteration: 2569, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003773805685341358
Iteration: 2570, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002132318913936615
Perfoming an random action
Iteration: 2571, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0001215711236000061
Iteration: 2572, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002526759635657072
Iteration: 2573, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013564582914113998
Collision detected, score: 0
Iteration: 2574, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0028316774405539036
Iteration: 2575, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001586531288921833
Iteration: 2576, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007047643419355154
Iteration: 2577, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006461231969296932
Iteration: 2578, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00595320388674736
Iteration: 2579, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 2580, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 2581, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 2582, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 2583, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 2584, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 2585, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 2586, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 2587, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 2588, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 2589, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 2590, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 2591, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 2592, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 2593, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 2594, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 2595, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 2596, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 2597, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001606294885277748
Iteration: 2598, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00021181628108024597
Iteration: 2599, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014240797609090805
Iteration: 2600, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025859763845801353
Collision detected, score: 0
Iteration: 2601, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.003087783232331276
Iteration: 2602, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000750761479139328
Iteration: 2603, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006587788928300142
Iteration: 2604, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047069573774933815
Iteration: 2605, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006763065233826637
Iteration: 2606, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2607, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2608, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2609, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2610, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2611, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2612, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2613, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2614, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2615, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2616, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2617, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2618, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2619, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2620, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2621, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2622, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2623, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2624, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018531465902924538
Iteration: 2625, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012809019535779953
Iteration: 2626, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006848126649856567
Iteration: 2627, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00029870402067899704
Iteration: 2628, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008225850760936737
Iteration: 2629, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -6.045214831829071e-05
Iteration: 2630, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023722900077700615
Iteration: 2631, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032378672622144222
Iteration: 2632, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0027193818241357803
Iteration: 2633, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005745962727814913
Iteration: 2634, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037477705627679825
Iteration: 2635, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00020041503012180328
Iteration: 2636, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031943675130605698
Iteration: 2637, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0002055121585726738
Iteration: 2638, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029977229423820972
Iteration: 2639, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 5.060620605945587e-05
Iteration: 2640, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017463602125644684
Iteration: 2641, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011202264577150345
Iteration: 2642, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00017888564616441727
Iteration: 2643, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017823241651058197
Iteration: 2644, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00039800070226192474
Iteration: 2645, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005916341673582792
Iteration: 2646, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003269865643233061
Iteration: 2647, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001505240797996521
Iteration: 2648, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003171435557305813
Iteration: 2649, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039560385048389435
Iteration: 2650, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008266827091574669
Iteration: 2651, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009923634119331837
Iteration: 2652, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004731595981866121
Iteration: 2653, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025090137496590614
Iteration: 2654, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021816371008753777
Iteration: 2655, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009426604956388474
Iteration: 2656, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0061495499685406685
Iteration: 2657, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005801330786198378
Iteration: 2658, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007417191751301289
Iteration: 2659, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004727292340248823
Iteration: 2660, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008982541039586067
Iteration: 2661, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040991404093801975
Iteration: 2662, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029850821010768414
Iteration: 2663, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004775729030370712
Iteration: 2664, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000767105259001255
Iteration: 2665, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006218523718416691
Iteration: 2666, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007944699376821518
Iteration: 2667, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005710614379495382
Collision detected, score: 0
Iteration: 2668, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.004462394397705793
Iteration: 2669, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024202303029596806
Iteration: 2670, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010240825824439526
Iteration: 2671, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007901137694716454
Iteration: 2672, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005392656195908785
Iteration: 2673, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2674, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2675, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2676, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2677, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2678, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2679, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2680, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2681, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2682, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2683, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2684, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2685, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2686, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2687, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2688, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2689, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2690, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2691, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025376491248607635
Iteration: 2692, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016734981909394264
Iteration: 2693, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000982559286057949
Iteration: 2694, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00010911747813224792
Iteration: 2695, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004999227821826935
Iteration: 2696, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000916721299290657
Iteration: 2697, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028637247160077095
Iteration: 2698, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026447558775544167
Iteration: 2699, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004616405814886093
Iteration: 2700, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00656320434063673
Iteration: 2701, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010854854248464108
Iteration: 2702, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004529157187789679
Iteration: 2703, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006117990240454674
Iteration: 2704, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0068731410428881645
Iteration: 2705, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005823121406137943
Iteration: 2706, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00692378543317318
Iteration: 2707, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0075413500890135765
Iteration: 2708, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005522344261407852
Iteration: 2709, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037240074016153812
Iteration: 2710, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004466000013053417
Iteration: 2711, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003037921153008938
Iteration: 2712, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006845214404165745
Iteration: 2713, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005607190076261759
Iteration: 2714, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00206802599132061
Iteration: 2715, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003540989477187395
Iteration: 2716, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019063204526901245
Iteration: 2717, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0065706814639270306
Iteration: 2718, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005995462648570538
Iteration: 2719, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012646596878767014
Iteration: 2720, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010049007833003998
Iteration: 2721, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003219330683350563
Iteration: 2722, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033832620829343796
Iteration: 2723, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005400834139436483
Iteration: 2724, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0048100026324391365
Iteration: 2725, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004479333758354187
Iteration: 2726, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004367424175143242
Iteration: 2727, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007061111740767956
Iteration: 2728, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008048815652728081
Iteration: 2729, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0015718601644039154
Collision detected, score: 0
Iteration: 2730, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002355411648750305
Iteration: 2731, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006632693111896515
Iteration: 2732, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004106274340301752
Iteration: 2733, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008073568344116211
Iteration: 2734, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006821022368967533
Iteration: 2735, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2736, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2737, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2738, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2739, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2740, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2741, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2742, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2743, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2744, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2745, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2746, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2747, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2748, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2749, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2750, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2751, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2752, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2753, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020369458943605423
Iteration: 2754, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001995147205889225
Iteration: 2755, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004830118268728256
Iteration: 2756, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006122719496488571
Iteration: 2757, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.3262338042259216e-05
Iteration: 2758, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007984582334756851
Iteration: 2759, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00034988950937986374
Iteration: 2760, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016390224918723106
Iteration: 2761, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028223302215337753
Iteration: 2762, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008430302143096924
Iteration: 2763, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009711290709674358
Iteration: 2764, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003922512289136648
Iteration: 2765, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008365372195839882
Iteration: 2766, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011943286284804344
Iteration: 2767, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009071690030395985
Iteration: 2768, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010089103132486343
Iteration: 2769, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009920267388224602
Iteration: 2770, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010462277568876743
Iteration: 2771, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006192117463797331
Iteration: 2772, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031372904777526855
Iteration: 2773, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006663443520665169
Iteration: 2774, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007786005735397339
Iteration: 2775, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008105304092168808
Iteration: 2776, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009393682703375816
Iteration: 2777, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005007996689528227
Iteration: 2778, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032560406252741814
Iteration: 2779, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006233099848031998
Iteration: 2780, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0002781488001346588
Iteration: 2781, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013067405670881271
Iteration: 2782, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002464843913912773
Iteration: 2783, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003692762926220894
Iteration: 2784, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0008420711383223534
Iteration: 2785, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002559200394898653
Iteration: 2786, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.001597665250301361
Iteration: 2787, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 7.680151611566544e-05
Iteration: 2788, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002228076569736004
Iteration: 2789, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023402394726872444
Iteration: 2790, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017388798296451569
Iteration: 2791, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002389158122241497
Iteration: 2792, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0023835990577936172
Iteration: 2793, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026178895495831966
Iteration: 2794, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029296232387423515
Iteration: 2795, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005274615250527859
Iteration: 2796, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007088671438395977
Iteration: 2797, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003090945538133383
Iteration: 2798, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005640281364321709
Collision detected, score: 0
Iteration: 2799, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.004070567898452282
Iteration: 2800, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005436040461063385
Iteration: 2801, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004492832813411951
Iteration: 2802, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006589720956981182
Iteration: 2803, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004203356336802244
Iteration: 2804, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2805, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2806, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2807, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2808, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2809, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2810, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2811, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2812, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2813, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2814, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2815, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2816, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2817, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2818, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2819, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2820, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2821, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2822, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020293621346354485
Iteration: 2823, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020269323140382767
Iteration: 2824, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006146933883428574
Iteration: 2825, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008388347923755646
Iteration: 2826, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000843060202896595
Iteration: 2827, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010893959552049637
Iteration: 2828, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008182954043149948
Iteration: 2829, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 8.24127346277237e-06
Iteration: 2830, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002666789572685957
Iteration: 2831, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006419033743441105
Iteration: 2832, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008857193402945995
Iteration: 2833, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005336197093129158
Iteration: 2834, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005215447396039963
Iteration: 2835, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011626235209405422
Iteration: 2836, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010186356492340565
Iteration: 2837, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00875011831521988
Iteration: 2838, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00530447019264102
Iteration: 2839, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010307615622878075
Iteration: 2840, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007255866192281246
Iteration: 2841, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010844000615179539
Iteration: 2842, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010721268132328987
Iteration: 2843, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011521657928824425
Iteration: 2844, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008823426440358162
Iteration: 2845, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009918782860040665
Iteration: 2846, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008091097697615623
Iteration: 2847, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027711917646229267
Iteration: 2848, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036452081985771656
Iteration: 2849, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037035876885056496
Iteration: 2850, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007712701335549355
Iteration: 2851, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015592128038406372
Iteration: 2852, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002939297817647457
Iteration: 2853, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002329038456082344
Collision detected, score: 0
Iteration: 2854, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0034949230030179024
Iteration: 2855, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031331866048276424
Iteration: 2856, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011220204643905163
Iteration: 2857, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022146226838231087
Iteration: 2858, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006692786701023579
Iteration: 2859, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2860, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2861, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2862, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2863, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2864, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2865, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2866, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2867, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2868, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2869, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2870, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2871, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2872, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2873, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2874, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2875, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2876, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2877, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018600989133119583
Iteration: 2878, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022421851754188538
Iteration: 2879, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011297594755887985
Iteration: 2880, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00042090192437171936
Iteration: 2881, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000662611797451973
Iteration: 2882, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.716457962989807e-06
Iteration: 2883, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006324872374534607
Iteration: 2884, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016753971576690674
Iteration: 2885, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003913456574082375
Iteration: 2886, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008012928068637848
Iteration: 2887, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008767693303525448
Iteration: 2888, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025344411842525005
Iteration: 2889, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007090373430401087
Iteration: 2890, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011775960214436054
Iteration: 2891, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009797710925340652
Iteration: 2892, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008780510164797306
Iteration: 2893, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012173955328762531
Iteration: 2894, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013569876551628113
Iteration: 2895, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010409950278699398
Iteration: 2896, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008458659052848816
Iteration: 2897, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.015467548742890358
Iteration: 2898, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012497510761022568
Iteration: 2899, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011563514359295368
Iteration: 2900, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012783296406269073
Iteration: 2901, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007960636168718338
Iteration: 2902, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005371034611016512
Iteration: 2903, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006285086739808321
Iteration: 2904, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006115676835179329
Iteration: 2905, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005562286823987961
Iteration: 2906, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005288776010274887
Iteration: 2907, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0026689358055591583
Iteration: 2908, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001041460782289505
Iteration: 2909, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0015317341312766075
Iteration: 2910, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0024267854169011116
Iteration: 2911, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017178189009428024
Iteration: 2912, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036739371716976166
Iteration: 2913, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012307940050959587
Iteration: 2914, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002837270498275757
Iteration: 2915, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010225661098957062
Iteration: 2916, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002351079136133194
Iteration: 2917, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0043172999285161495
Iteration: 2918, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031870510429143906
Iteration: 2919, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015317462384700775
Iteration: 2920, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002426440827548504
Iteration: 2921, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023970818147063255
Iteration: 2922, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006057639140635729
Collision detected, score: 0
Iteration: 2923, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.005000723991543055
Iteration: 2924, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025463318452239037
Iteration: 2925, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005219361744821072
Iteration: 2926, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008727224543690681
Iteration: 2927, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006863483227789402
Iteration: 2928, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 2929, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 2930, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 2931, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 2932, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 2933, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 2934, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 2935, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 2936, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 2937, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 2938, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 2939, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 2940, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 2941, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 2942, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 2943, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 2944, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 2945, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 2946, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001414499245584011
Iteration: 2947, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016331197693943977
Iteration: 2948, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007655909284949303
Iteration: 2949, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -6.84242695569992e-05
Iteration: 2950, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012330180034041405
Iteration: 2951, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011930083855986595
Iteration: 2952, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004910637624561787
Iteration: 2953, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005199766717851162
Iteration: 2954, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003952204715460539
Iteration: 2955, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007476353086531162
Iteration: 2956, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004405787214636803
Iteration: 2957, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025186496786773205
Iteration: 2958, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003752235323190689
Iteration: 2959, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025916146114468575
Iteration: 2960, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013117780908942223
Iteration: 2961, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002848176285624504
Iteration: 2962, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038638757541775703
Iteration: 2963, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030844085849821568
Iteration: 2964, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003720499575138092
Iteration: 2965, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003949162550270557
Iteration: 2966, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005192945245653391
Iteration: 2967, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006352083757519722
Iteration: 2968, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006586252246052027
Iteration: 2969, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028244969435036182
Iteration: 2970, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010659852996468544
Iteration: 2971, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020822742953896523
Iteration: 2972, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009814940392971039
Iteration: 2973, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008553994819521904
Iteration: 2974, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008315186947584152
Iteration: 2975, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035501248203217983
Iteration: 2976, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004204999189823866
Iteration: 2977, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005123543087393045
Iteration: 2978, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006189840845763683
Iteration: 2979, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017641400918364525
Iteration: 2980, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029626572504639626
Iteration: 2981, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033954326063394547
Iteration: 2982, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002359982579946518
Iteration: 2983, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004901760257780552
Iteration: 2984, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012922566384077072
Iteration: 2985, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011414363980293274
Iteration: 2986, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004567435011267662
Iteration: 2987, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005720846354961395
Iteration: 2988, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008621739223599434
Iteration: 2989, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007335726637393236
Iteration: 2990, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0068421075120568275
Iteration: 2991, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008733481168746948
Iteration: 2992, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007963702082633972
Iteration: 2993, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004426632542163134
Iteration: 2994, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011172926984727383
Iteration: 2995, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006565978284925222
Iteration: 2996, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0059663355350494385
Iteration: 2997, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00499791419133544
Iteration: 2998, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00036034639924764633
Iteration: 2999, learning state: observe, epsilon: 0.0075, did jump: No, reward: 1, Q_max: 0.0056174444034695625
Iteration: 3000, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013926215469837189
Iteration: 3001, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005914140492677689
Iteration: 3002, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014492310583591461
Collision detected, score: 0
Iteration: 3003, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.007362745702266693
Iteration: 3004, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013704702258110046
Iteration: 3005, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008731537498533726
Iteration: 3006, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008705038577318192
Iteration: 3007, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005441674031317234
Iteration: 3008, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3009, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00330183282494545
Iteration: 3010, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002642674371600151
Iteration: 3011, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0009143836796283722
Iteration: 3012, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008190767839550972
Iteration: 3013, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007849046960473061
Iteration: 3014, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001066630706191063
Iteration: 3015, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 5.462300032377243e-05
Iteration: 3016, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002236592583358288
Iteration: 3017, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037690591998398304
Iteration: 3018, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005692086648195982
Iteration: 3019, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004414697643369436
Perfoming an random action
Iteration: 3020, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023437347263097763
Iteration: 3021, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002179504372179508
Iteration: 3022, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0002999426797032356
Iteration: 3023, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002160165458917618
Iteration: 3024, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007373783737421036
Iteration: 3025, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002501250244677067
Iteration: 3026, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007684612646698952
Iteration: 3027, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017250534147024155
Iteration: 3028, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031585474498569965
Iteration: 3029, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003203685861080885
Iteration: 3030, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002737813163548708
Iteration: 3031, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003421683330088854
Perfoming an random action
Iteration: 3032, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006383317057043314
Iteration: 3033, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007912730798125267
Iteration: 3034, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0035678017884492874
Iteration: 3035, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003442356362938881
Iteration: 3036, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023938268423080444
Collision detected, score: 0
Iteration: 3037, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: -0.0005601514130830765
Iteration: 3038, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005733584985136986
Iteration: 3039, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003626205027103424
Iteration: 3040, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037669530138373375
Iteration: 3041, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002320345491170883
Iteration: 3042, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 3043, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 3044, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 3045, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 3046, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 3047, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 3048, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 3049, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 3050, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 3051, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 3052, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 3053, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 3054, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 3055, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 3056, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 3057, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 3058, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 3059, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 3060, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005919190123677254
Iteration: 3061, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038744425401091576
Perfoming an random action
Iteration: 3062, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037511405535042286
Iteration: 3063, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002664969302713871
Iteration: 3064, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00328215304762125
Iteration: 3065, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006680023856461048
Iteration: 3066, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0040357974357903
Iteration: 3067, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009561125189065933
Iteration: 3068, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007015093229711056
Iteration: 3069, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008145072497427464
Iteration: 3070, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007140228524804115
Iteration: 3071, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008786752820014954
Iteration: 3072, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005868284031748772
Iteration: 3073, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009723637253046036
Iteration: 3074, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007721113972365856
Iteration: 3075, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004574884660542011
Iteration: 3076, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006008762866258621
Iteration: 3077, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003732732031494379
Iteration: 3078, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001967746764421463
Iteration: 3079, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025228578597307205
Iteration: 3080, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0042831674218177795
Iteration: 3081, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005828081164509058
Iteration: 3082, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007684077601879835
Iteration: 3083, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004620692692697048
Iteration: 3084, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000816945917904377
Iteration: 3085, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016432423144578934
Iteration: 3086, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002085532061755657
Iteration: 3087, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015011224895715714
Iteration: 3088, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003229137510061264
Iteration: 3089, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015648184344172478
Iteration: 3090, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00015261583030223846
Iteration: 3091, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015654927119612694
Iteration: 3092, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038331886753439903
Iteration: 3093, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00433867983520031
Iteration: 3094, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0027843965217471123
Iteration: 3095, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004972514696419239
Iteration: 3096, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003032519482076168
Collision detected, score: 0
Iteration: 3097, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.002322666347026825
Iteration: 3098, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006306087598204613
Iteration: 3099, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024921810254454613
Iteration: 3100, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030855583027005196
Iteration: 3101, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012012496590614319
Iteration: 3102, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 3103, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017140824347734451
Iteration: 3104, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016389107331633568
Iteration: 3105, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014947187155485153
Iteration: 3106, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001966129057109356
Iteration: 3107, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004209826700389385
Iteration: 3108, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004745318088680506
Iteration: 3109, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003355017863214016
Iteration: 3110, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004265457391738892
Iteration: 3111, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004399097058922052
Iteration: 3112, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004322832450270653
Iteration: 3113, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019652200862765312
Iteration: 3114, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001681785099208355
Iteration: 3115, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006626388058066368
Iteration: 3116, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003104171250015497
Iteration: 3117, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034972475841641426
Iteration: 3118, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008184014819562435
Iteration: 3119, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00528718251734972
Iteration: 3120, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005092327482998371
Iteration: 3121, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003738135565072298
Iteration: 3122, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038797766901552677
Iteration: 3123, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006809558719396591
Iteration: 3124, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002609106246381998
Iteration: 3125, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005814550444483757
Iteration: 3126, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0064934371039271355
Iteration: 3127, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003058128524571657
Iteration: 3128, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003552015870809555
Iteration: 3129, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006989056244492531
Iteration: 3130, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008812307380139828
Iteration: 3131, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012995411641895771
Iteration: 3132, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010208340361714363
Iteration: 3133, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009410425089299679
Iteration: 3134, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007942106574773788
Iteration: 3135, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031513599678874016
Iteration: 3136, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030712513253092766
Iteration: 3137, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002750767394900322
Iteration: 3138, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005119167268276215
Iteration: 3139, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037231645546853542
Iteration: 3140, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0054776910692453384
Iteration: 3141, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006000751629471779
Iteration: 3142, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004964432679116726
Iteration: 3143, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034685786813497543
Iteration: 3144, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004137549549341202
Iteration: 3145, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010441262274980545
Iteration: 3146, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006732250098139048
Iteration: 3147, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006464370992034674
Iteration: 3148, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030047683976590633
Iteration: 3149, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005784798413515091
Collision detected, score: 0
Iteration: 3150, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0019005555659532547
Iteration: 3151, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025984379462897778
Iteration: 3152, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010423321276903152
Iteration: 3153, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006645449437201023
Iteration: 3154, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002316676080226898
Iteration: 3155, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 3156, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 3157, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 3158, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 3159, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 3160, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 3161, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 3162, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 3163, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 3164, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 3165, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 3166, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 3167, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 3168, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 3169, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 3170, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 3171, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 3172, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 3173, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014973515644669533
Iteration: 3174, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0004345159977674484
Iteration: 3175, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00352596677839756
Iteration: 3176, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018739979714155197
Collision detected, score: 0
Iteration: 3177, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00023938901722431183
Iteration: 3178, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032921279780566692
Iteration: 3179, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040130396373569965
Iteration: 3180, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006134202238172293
Iteration: 3181, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005094613879919052
Iteration: 3182, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 3183, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Perfoming an random action
Iteration: 3184, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 3185, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 3186, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 3187, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 3188, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 3189, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 3190, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 3191, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 3192, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 3193, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 3194, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 3195, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 3196, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 3197, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 3198, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 3199, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 3200, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005553067196160555
Iteration: 3201, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003986528143286705
Iteration: 3202, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038067051209509373
Iteration: 3203, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027404525317251682
Iteration: 3204, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031383181922137737
Iteration: 3205, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005070963408797979
Iteration: 3206, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012738145887851715
Iteration: 3207, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006841578520834446
Iteration: 3208, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033357935026288033
Iteration: 3209, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0041475966572761536
Iteration: 3210, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027896231040358543
Iteration: 3211, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024402225390076637
Iteration: 3212, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023597790859639645
Iteration: 3213, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006183918099850416
Iteration: 3214, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005830396898090839
Iteration: 3215, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006296738516539335
Iteration: 3216, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031558582559227943
Iteration: 3217, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033515398390591145
Iteration: 3218, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005678844638168812
Iteration: 3219, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007029326632618904
Iteration: 3220, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005156226921826601
Iteration: 3221, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003864550031721592
Iteration: 3222, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005642352160066366
Iteration: 3223, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003378031775355339
Iteration: 3224, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00048013217747211456
Iteration: 3225, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022585224360227585
Iteration: 3226, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0004428951069712639
Iteration: 3227, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00047150999307632446
Iteration: 3228, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012241750955581665
Iteration: 3229, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007505491375923157
Collision detected, score: 0
Iteration: 3230, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0037945727817714214
Iteration: 3231, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001802734099328518
Iteration: 3232, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01568593457341194
Iteration: 3233, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009883299469947815
Iteration: 3234, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006834408268332481
Iteration: 3235, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3236, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 3237, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 3238, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 3239, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 3240, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 3241, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 3242, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 3243, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 3244, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 3245, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 3246, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 3247, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 3248, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 3249, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 3250, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 3251, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 3252, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 3253, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018087150529026985
Iteration: 3254, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019068662077188492
Iteration: 3255, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006445180624723434
Iteration: 3256, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012279385700821877
Iteration: 3257, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -9.982939809560776e-05
Iteration: 3258, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012739989906549454
Iteration: 3259, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011233827099204063
Iteration: 3260, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010272208601236343
Iteration: 3261, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025496608577668667
Iteration: 3262, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0074266595765948296
Iteration: 3263, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010473358444869518
Iteration: 3264, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004057594574987888
Iteration: 3265, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00901002250611782
Iteration: 3266, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010558158159255981
Iteration: 3267, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007530795410275459
Iteration: 3268, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009887796826660633
Iteration: 3269, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011255340650677681
Iteration: 3270, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011052667163312435
Iteration: 3271, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008485271595418453
Iteration: 3272, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0065968651324510574
Iteration: 3273, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009565848857164383
Iteration: 3274, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008627602830529213
Iteration: 3275, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009675277397036552
Iteration: 3276, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008983242325484753
Iteration: 3277, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006123445462435484
Iteration: 3278, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006279150955379009
Iteration: 3279, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00553316343575716
Iteration: 3280, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004391779657453299
Iteration: 3281, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004609248600900173
Iteration: 3282, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010491842404007912
Iteration: 3283, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0037317555397748947
Iteration: 3284, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0013777585700154305
Iteration: 3285, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006336187943816185
Iteration: 3286, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00068639125674963
Iteration: 3287, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0003692992031574249
Iteration: 3288, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001838313415646553
Iteration: 3289, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007602032274007797
Iteration: 3290, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0012445589527487755
Iteration: 3291, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0023459522053599358
Iteration: 3292, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001380843110382557
Iteration: 3293, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029956549406051636
Iteration: 3294, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032406337559223175
Iteration: 3295, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014872485771775246
Iteration: 3296, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004920965991914272
Iteration: 3297, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020033344626426697
Iteration: 3298, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004383684601634741
Collision detected, score: 0
Iteration: 3299, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.005518908612430096
Iteration: 3300, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004646812565624714
Iteration: 3301, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005323708523064852
Iteration: 3302, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007523265667259693
Iteration: 3303, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004203356336802244
Iteration: 3304, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3305, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 3306, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 3307, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 3308, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 3309, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 3310, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 3311, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 3312, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 3313, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 3314, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 3315, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 3316, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 3317, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 3318, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 3319, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 3320, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 3321, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 3322, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001832805573940277
Iteration: 3323, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016009295359253883
Iteration: 3324, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009548971429467201
Iteration: 3325, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008276952430605888
Iteration: 3326, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007136045023798943
Iteration: 3327, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -5.056988447904587e-05
Iteration: 3328, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002870284952223301
Iteration: 3329, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0054634748958051205
Iteration: 3330, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002694049384444952
Iteration: 3331, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008510683663189411
Iteration: 3332, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0068254852667450905
Iteration: 3333, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004060001578181982
Iteration: 3334, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007655689027160406
Iteration: 3335, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005619407631456852
Iteration: 3336, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0063451677560806274
Iteration: 3337, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006401478312909603
Iteration: 3338, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008781413547694683
Iteration: 3339, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007806035690009594
Iteration: 3340, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006200993433594704
Iteration: 3341, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008124924264848232
Iteration: 3342, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0055985236540436745
Iteration: 3343, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005501959938555956
Iteration: 3344, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005314631853252649
Iteration: 3345, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006632044445723295
Iteration: 3346, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005716889165341854
Iteration: 3347, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008329203352332115
Iteration: 3348, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00473968917503953
Iteration: 3349, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004641413688659668
Iteration: 3350, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037321443669497967
Iteration: 3351, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011933716014027596
Iteration: 3352, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012267474085092545
Iteration: 3353, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003864523023366928
Iteration: 3354, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -9.849853813648224e-05
Iteration: 3355, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002273908816277981
Iteration: 3356, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034753205254673958
Iteration: 3357, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023272098042070866
Iteration: 3358, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001710747368633747
Iteration: 3359, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003774834331125021
Iteration: 3360, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00031658541411161423
Iteration: 3361, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010831216350197792
Iteration: 3362, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006331922486424446
Iteration: 3363, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004601422231644392
Iteration: 3364, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0027298619970679283
Iteration: 3365, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002039693295955658
Iteration: 3366, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00011982396245002747
Iteration: 3367, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001046723686158657
Iteration: 3368, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002420637756586075
Iteration: 3369, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023475317284464836
Iteration: 3370, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003951519727706909
Iteration: 3371, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004328045062720776
Iteration: 3372, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007006403058767319
Iteration: 3373, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0063497391529381275
Iteration: 3374, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0004667174071073532
Iteration: 3375, learning state: observe, epsilon: 0.0075, did jump: No, reward: 1, Q_max: 0.005167832598090172
Iteration: 3376, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003482334315776825
Iteration: 3377, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004217863082885742
Iteration: 3378, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0035773799754679203
Iteration: 3379, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00277921836823225
Iteration: 3380, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008646950125694275
Iteration: 3381, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00478710513561964
Collision detected, score: 0
Iteration: 3382, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0017949724569916725
Iteration: 3383, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004325127229094505
Iteration: 3384, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006602770183235407
Iteration: 3385, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010272914543747902
Iteration: 3386, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.009468376636505127
Iteration: 3387, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3388, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00330183282494545
Iteration: 3389, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002642674371600151
Iteration: 3390, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0009143836796283722
Iteration: 3391, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008190767839550972
Iteration: 3392, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007849046960473061
Iteration: 3393, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001066630706191063
Iteration: 3394, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 5.462300032377243e-05
Iteration: 3395, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002236592583358288
Iteration: 3396, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037690591998398304
Iteration: 3397, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005692086648195982
Iteration: 3398, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004414697643369436
Iteration: 3399, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023437347263097763
Iteration: 3400, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002179504372179508
Iteration: 3401, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0002999426797032356
Iteration: 3402, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002160165458917618
Iteration: 3403, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007373783737421036
Iteration: 3404, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002501250244677067
Iteration: 3405, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013992507010698318
Iteration: 3406, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014495719224214554
Iteration: 3407, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033360635861754417
Iteration: 3408, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002764811273664236
Iteration: 3409, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015749381855130196
Iteration: 3410, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030766716226935387
Iteration: 3411, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006608778145164251
Iteration: 3412, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006935723125934601
Iteration: 3413, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004064003471285105
Iteration: 3414, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006684956606477499
Iteration: 3415, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0072932373732328415
Collision detected, score: 0
Iteration: 3416, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00810310523957014
Iteration: 3417, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006092286668717861
Iteration: 3418, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0071625192649662495
Iteration: 3419, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004612802527844906
Iteration: 3420, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008364342153072357
Iteration: 3421, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 3422, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 3423, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 3424, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 3425, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 3426, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 3427, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 3428, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 3429, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 3430, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 3431, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 3432, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 3433, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 3434, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 3435, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 3436, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 3437, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 3438, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 3439, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001686571165919304
Iteration: 3440, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005366150289773941
Iteration: 3441, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015139486640691757
Iteration: 3442, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030485698953270912
Collision detected, score: 0
Iteration: 3443, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0024178866297006607
Iteration: 3444, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012457910925149918
Iteration: 3445, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006416721735149622
Iteration: 3446, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005068617407232523
Iteration: 3447, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006666284520179033
Iteration: 3448, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3449, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 3450, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 3451, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 3452, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 3453, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 3454, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 3455, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 3456, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 3457, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 3458, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 3459, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 3460, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 3461, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 3462, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 3463, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 3464, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 3465, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 3466, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019114790484309196
Iteration: 3467, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014735013246536255
Iteration: 3468, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008898554369807243
Iteration: 3469, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006386330351233482
Iteration: 3470, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006592180579900742
Iteration: 3471, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006655603647232056
Iteration: 3472, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030557396821677685
Iteration: 3473, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030117444694042206
Iteration: 3474, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024966630153357983
Iteration: 3475, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006366925314068794
Iteration: 3476, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032870564609766006
Iteration: 3477, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005232058465480804
Iteration: 3478, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024938806891441345
Iteration: 3479, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.583783447742462e-05
Iteration: 3480, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0038020936772227287
Iteration: 3481, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020216889679431915
Iteration: 3482, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022844672203063965
Iteration: 3483, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0003057653084397316
Iteration: 3484, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002016602084040642
Iteration: 3485, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037999670021235943
Iteration: 3486, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033980002626776695
Iteration: 3487, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007643651217222214
Iteration: 3488, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005518287420272827
Iteration: 3489, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030275932513177395
Iteration: 3490, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002371099777519703
Iteration: 3491, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00213688425719738
Iteration: 3492, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006721125915646553
Iteration: 3493, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009222092106938362
Iteration: 3494, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039461092092096806
Iteration: 3495, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012980606406927109
Iteration: 3496, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024649309925734997
Iteration: 3497, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012738490477204323
Iteration: 3498, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005183128640055656
Iteration: 3499, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005088718608021736
Iteration: 3500, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006169255822896957
Iteration: 3501, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004213147796690464
Iteration: 3502, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004896866157650948
Iteration: 3503, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020041577517986298
Iteration: 3504, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002911727875471115
Iteration: 3505, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00224970281124115
Iteration: 3506, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017252825200557709
Iteration: 3507, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004071120172739029
Iteration: 3508, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011468036100268364
Iteration: 3509, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00817786157131195
Collision detected, score: 0
Iteration: 3510, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.006375714670866728
Iteration: 3511, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042568896897137165
Iteration: 3512, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011086245067417622
Iteration: 3513, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007955116219818592
Iteration: 3514, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007504034321755171
Iteration: 3515, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3516, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 3517, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 3518, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 3519, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 3520, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 3521, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 3522, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 3523, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 3524, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 3525, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 3526, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 3527, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 3528, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 3529, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 3530, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 3531, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 3532, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 3533, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015427414327859879
Iteration: 3534, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017667338252067566
Iteration: 3535, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000483805313706398
Iteration: 3536, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -1.905485987663269e-05
Iteration: 3537, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00039334502071142197
Iteration: 3538, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.001410502940416336
Iteration: 3539, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005706753581762314
Iteration: 3540, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031474814750254154
Iteration: 3541, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002163931727409363
Iteration: 3542, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00842650793492794
Iteration: 3543, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007381357252597809
Iteration: 3544, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005715731531381607
Iteration: 3545, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007310300134122372
Iteration: 3546, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007017691619694233
Iteration: 3547, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008681976236402988
Iteration: 3548, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005086554680019617
Iteration: 3549, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008911429904401302
Iteration: 3550, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0115616200491786
Iteration: 3551, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011735198087990284
Iteration: 3552, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01253066211938858
Iteration: 3553, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012877209112048149
Iteration: 3554, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008532562293112278
Iteration: 3555, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01003707479685545
Iteration: 3556, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010297829285264015
Iteration: 3557, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010400155559182167
Iteration: 3558, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008614522404968739
Iteration: 3559, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007381021045148373
Iteration: 3560, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010480151511728764
Iteration: 3561, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006544029340147972
Iteration: 3562, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0062557668425142765
Iteration: 3563, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002736000344157219
Iteration: 3564, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0045046256855130196
Iteration: 3565, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00021049194037914276
Iteration: 3566, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0020828694105148315
Iteration: 3567, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0020209811627864838
Iteration: 3568, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008753808215260506
Iteration: 3569, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00019521452486515045
Iteration: 3570, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00010025035589933395
Iteration: 3571, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008133603259921074
Iteration: 3572, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011387104168534279
Iteration: 3573, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006706319749355316
Iteration: 3574, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005398628301918507
Iteration: 3575, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012136641889810562
Collision detected, score: 0
Iteration: 3576, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0002583339810371399
Iteration: 3577, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014499938115477562
Iteration: 3578, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029956456273794174
Iteration: 3579, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00815363135188818
Iteration: 3580, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003494421485811472
Iteration: 3581, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 3582, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 3583, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 3584, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 3585, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 3586, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 3587, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 3588, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 3589, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 3590, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 3591, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 3592, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 3593, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 3594, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 3595, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 3596, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 3597, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 3598, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 3599, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890774428844452
Iteration: 3600, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029402077198028564
Iteration: 3601, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004585767164826393
Collision detected, score: 0
Iteration: 3602, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002681498881429434
Iteration: 3603, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008698189631104469
Iteration: 3604, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00800113845616579
Iteration: 3605, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007719344925135374
Iteration: 3606, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0063875587657094
Iteration: 3607, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 3608, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 3609, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 3610, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 3611, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 3612, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 3613, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 3614, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 3615, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 3616, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 3617, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 3618, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 3619, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 3620, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 3621, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 3622, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 3623, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 3624, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 3625, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014288239181041718
Iteration: 3626, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0001857047900557518
Iteration: 3627, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003218303434550762
Iteration: 3628, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029182564467191696
Collision detected, score: 0
Iteration: 3629, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0033782916143536568
Iteration: 3630, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00268751522526145
Iteration: 3631, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006391205359250307
Iteration: 3632, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003953661769628525
Iteration: 3633, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006350154057145119
Iteration: 3634, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 3635, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 3636, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 3637, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 3638, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 3639, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 3640, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 3641, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 3642, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 3643, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 3644, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 3645, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 3646, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 3647, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 3648, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 3649, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 3650, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 3651, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 3652, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014744261279702187
Iteration: 3653, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 7.38808885216713e-05
Iteration: 3654, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032652621157467365
Iteration: 3655, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002626369707286358
Collision detected, score: 0
Iteration: 3656, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.003325236961245537
Iteration: 3657, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003051110077649355
Iteration: 3658, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006516704801470041
Iteration: 3659, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004877595230937004
Iteration: 3660, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00026054494082927704
Iteration: 3661, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 3662, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 3663, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 3664, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 3665, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 3666, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 3667, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 3668, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 3669, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 3670, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 3671, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 3672, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 3673, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 3674, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 3675, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 3676, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 3677, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 3678, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 3679, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005730991251766682
Iteration: 3680, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040606483817100525
Iteration: 3681, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004398838616907597
Iteration: 3682, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029008975252509117
Iteration: 3683, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003957611974328756
Iteration: 3684, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006071576848626137
Iteration: 3685, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0042586256749928
Iteration: 3686, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.010321730747818947
Iteration: 3687, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005726141389459372
Iteration: 3688, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00550311803817749
Iteration: 3689, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007537988945841789
Iteration: 3690, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006160167045891285
Iteration: 3691, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005083519034087658
Iteration: 3692, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007422054186463356
Iteration: 3693, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0062936414033174515
Iteration: 3694, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007232981733977795
Iteration: 3695, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005990508943796158
Iteration: 3696, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025977594777941704
Iteration: 3697, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023506684228777885
Iteration: 3698, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031015058048069477
Iteration: 3699, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014958400279283524
Iteration: 3700, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020490409806370735
Iteration: 3701, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031021428294479847
Iteration: 3702, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003308443818241358
Iteration: 3703, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020138323307037354
Iteration: 3704, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005096419248729944
Collision detected, score: 0
Iteration: 3705, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0074657537043094635
Iteration: 3706, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005222258158028126
Iteration: 3707, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007396849803626537
Iteration: 3708, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036557097919285297
Iteration: 3709, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000680241733789444
Iteration: 3710, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005756663158535957
Iteration: 3711, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0016825413331389427
Iteration: 3712, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0017319228500127792
Iteration: 3713, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010202908888459206
Iteration: 3714, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001914030872285366
Iteration: 3715, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014398479834198952
Iteration: 3716, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003003037068992853
Iteration: 3717, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00649223430082202
Iteration: 3718, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037936223670840263
Iteration: 3719, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021955566480755806
Iteration: 3720, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0022475747391581535
Iteration: 3721, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00041833706200122833
Iteration: 3722, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014485130086541176
Iteration: 3723, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006245449185371399
Iteration: 3724, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024553160183131695
Iteration: 3725, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011736582964658737
Iteration: 3726, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0019117221236228943
Iteration: 3727, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017217788845300674
Iteration: 3728, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014687823131680489
Iteration: 3729, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010941792279481888
Iteration: 3730, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00360837671905756
Iteration: 3731, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024289479479193687
Iteration: 3732, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019230740144848824
Iteration: 3733, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004112047143280506
Iteration: 3734, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00263822078704834
Iteration: 3735, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0056601776741445065
Iteration: 3736, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004981664475053549
Collision detected, score: 0
Iteration: 3737, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.005046787206083536
Iteration: 3738, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008398015983402729
Iteration: 3739, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010430247522890568
Iteration: 3740, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025989944115281105
Iteration: 3741, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007264205254614353
Iteration: 3742, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 3743, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 3744, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 3745, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 3746, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 3747, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 3748, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 3749, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 3750, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 3751, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 3752, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 3753, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 3754, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 3755, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 3756, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 3757, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 3758, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 3759, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 3760, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015992233529686928
Iteration: 3761, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00017124321311712265
Iteration: 3762, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003064536489546299
Iteration: 3763, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002418520860373974
Collision detected, score: 0
Iteration: 3764, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0031375885009765625
Iteration: 3765, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025349045172333717
Iteration: 3766, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006574027705937624
Iteration: 3767, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004929930903017521
Iteration: 3768, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0007470138370990753
Iteration: 3769, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 3770, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -9.88394021987915e-05
Iteration: 3771, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00020933710038661957
Iteration: 3772, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012660948559641838
Iteration: 3773, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018334342166781425
Iteration: 3774, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0044052572920918465
Iteration: 3775, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005087495781481266
Iteration: 3776, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028295740485191345
Iteration: 3777, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036235665902495384
Iteration: 3778, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011557694524526596
Iteration: 3779, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002689965069293976
Iteration: 3780, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00020991358906030655
Iteration: 3781, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00014249607920646667
Iteration: 3782, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002909666858613491
Iteration: 3783, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013370774686336517
Iteration: 3784, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003773805685341358
Iteration: 3785, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002132318913936615
Iteration: 3786, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0001215711236000061
Iteration: 3787, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002212952356785536
Iteration: 3788, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016564764082431793
Collision detected, score: 0
Iteration: 3789, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0007095476612448692
Iteration: 3790, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017732251435518265
Iteration: 3791, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0056934235617518425
Iteration: 3792, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040294937789440155
Iteration: 3793, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007238172926008701
Perfoming an random action
Iteration: 3794, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3795, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 3796, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 3797, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 3798, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 3799, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 3800, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 3801, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 3802, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 3803, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 3804, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 3805, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 3806, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 3807, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 3808, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 3809, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 3810, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 3811, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 3812, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001779012382030487
Iteration: 3813, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002470920328050852
Iteration: 3814, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013988027349114418
Iteration: 3815, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007868213579058647
Iteration: 3816, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000357605516910553
Iteration: 3817, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011780215427279472
Iteration: 3818, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001261010766029358
Iteration: 3819, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012030862271785736
Iteration: 3820, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001798330806195736
Iteration: 3821, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009513487108051777
Iteration: 3822, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005652664229273796
Iteration: 3823, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005524363834410906
Iteration: 3824, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008570102974772453
Iteration: 3825, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0072490498423576355
Iteration: 3826, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008791334927082062
Iteration: 3827, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005526057910174131
Iteration: 3828, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01140808966010809
Iteration: 3829, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014980100095272064
Iteration: 3830, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012013610452413559
Iteration: 3831, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011590363457798958
Iteration: 3832, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010712915100157261
Iteration: 3833, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013233385048806667
Iteration: 3834, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011079961434006691
Iteration: 3835, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011814592406153679
Iteration: 3836, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011530008167028427
Iteration: 3837, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004075660835951567
Iteration: 3838, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007835903204977512
Iteration: 3839, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007580493111163378
Iteration: 3840, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009437006898224354
Iteration: 3841, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021455055102705956
Iteration: 3842, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0033026495948433876
Iteration: 3843, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.004241630434989929
Iteration: 3844, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005058282986283302
Iteration: 3845, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0018937978893518448
Iteration: 3846, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00010935217142105103
Iteration: 3847, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007641389966011047
Iteration: 3848, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0025635119527578354
Iteration: 3849, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0018275575712323189
Iteration: 3850, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0010250229388475418
Iteration: 3851, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0012641362845897675
Iteration: 3852, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007811699528247118
Iteration: 3853, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028851013630628586
Iteration: 3854, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018814755603671074
Iteration: 3855, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00037502311170101166
Iteration: 3856, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00024762190878391266
Iteration: 3857, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004548937547951937
Collision detected, score: 0
Iteration: 3858, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.004526673350483179
Iteration: 3859, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013844454661011696
Iteration: 3860, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032665589824318886
Iteration: 3861, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008740952238440514
Iteration: 3862, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007387436460703611
Iteration: 3863, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 3864, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 3865, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 3866, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 3867, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 3868, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 3869, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 3870, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 3871, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 3872, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 3873, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 3874, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 3875, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 3876, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 3877, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 3878, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 3879, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 3880, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 3881, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001695837825536728
Iteration: 3882, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002010321244597435
Iteration: 3883, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007841596379876137
Iteration: 3884, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013467362150549889
Iteration: 3885, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 8.258502930402756e-05
Iteration: 3886, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002651531249284744
Iteration: 3887, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025829439982771873
Iteration: 3888, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004864262882620096
Iteration: 3889, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002692508976906538
Iteration: 3890, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008826461620628834
Iteration: 3891, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007131711579859257
Iteration: 3892, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0052335914224386215
Iteration: 3893, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008671908639371395
Iteration: 3894, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007427299860864878
Iteration: 3895, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008321276865899563
Iteration: 3896, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008039064705371857
Iteration: 3897, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007065498270094395
Iteration: 3898, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0065171979367733
Iteration: 3899, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00847227219492197
Iteration: 3900, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01074489951133728
Iteration: 3901, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008055808953940868
Iteration: 3902, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006740964483469725
Iteration: 3903, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005511293653398752
Iteration: 3904, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005573654547333717
Perfoming an random action
Iteration: 3905, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004608366172760725
Iteration: 3906, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006051019299775362
Iteration: 3907, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007095795124769211
Iteration: 3908, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007769763469696045
Iteration: 3909, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008117806166410446
Iteration: 3910, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005843870807439089
Iteration: 3911, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004280490335077047
Iteration: 3912, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0073531391099095345
Iteration: 3913, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009665966965258121
Iteration: 3914, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006940723396837711
Iteration: 3915, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009364285506308079
Iteration: 3916, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032976954244077206
Iteration: 3917, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012902813032269478
Iteration: 3918, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026191119104623795
Iteration: 3919, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0002795560285449028
Iteration: 3920, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014271223917603493
Iteration: 3921, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007374174892902374
Iteration: 3922, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0022542974911630154
Iteration: 3923, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0026517934165894985
Iteration: 3924, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.001539980061352253
Collision detected, score: 0
Iteration: 3925, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: -0.0026321643963456154
Iteration: 3926, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0015771854668855667
Iteration: 3927, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00023019127547740936
Iteration: 3928, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034256307408213615
Iteration: 3929, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00036044325679540634
Iteration: 3930, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00019875355064868927
Iteration: 3931, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007875030860304832
Iteration: 3932, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006735948845744133
Iteration: 3933, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.103927105665207e-05
Iteration: 3934, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016052694991230965
Iteration: 3935, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025912243872880936
Iteration: 3936, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005807065404951572
Iteration: 3937, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004960022401064634
Iteration: 3938, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004619362764060497
Iteration: 3939, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029010260477662086
Iteration: 3940, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002361895516514778
Iteration: 3941, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00293709896504879
Iteration: 3942, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0044938973151147366
Iteration: 3943, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005802833940833807
Iteration: 3944, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020632771775126457
Iteration: 3945, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004071156959980726
Iteration: 3946, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00413003982976079
Iteration: 3947, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005140780005604029
Iteration: 3948, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006330991163849831
Iteration: 3949, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0049499874003231525
Iteration: 3950, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030748359858989716
Iteration: 3951, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005882914178073406
Iteration: 3952, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031559783965349197
Iteration: 3953, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003037654794752598
Iteration: 3954, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027483366429805756
Iteration: 3955, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002164030447602272
Iteration: 3956, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0051775528118014336
Iteration: 3957, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005813403055071831
Iteration: 3958, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006120000965893269
Iteration: 3959, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00383269926533103
Iteration: 3960, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006671604700386524
Iteration: 3961, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008318641223013401
Iteration: 3962, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009209562093019485
Iteration: 3963, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008512764237821102
Iteration: 3964, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009674121625721455
Iteration: 3965, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012052501551806927
Iteration: 3966, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010304148308932781
Iteration: 3967, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011727286502718925
Iteration: 3968, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012834247201681137
Iteration: 3969, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012232847511768341
Iteration: 3970, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010458013042807579
Iteration: 3971, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009274488314986229
Iteration: 3972, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005532710812985897
Iteration: 3973, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002587126102298498
Iteration: 3974, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0040367101319134235
Iteration: 3975, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038099223747849464
Iteration: 3976, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034434893168509007
Iteration: 3977, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001706419512629509
Iteration: 3978, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007434524595737457
Iteration: 3979, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0035901283845305443
Collision detected, score: 0
Iteration: 3980, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0027216756716370583
Iteration: 3981, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002343997359275818
Iteration: 3982, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008639023639261723
Iteration: 3983, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003008066676557064
Iteration: 3984, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006346131209284067
Iteration: 3985, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 3986, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 3987, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 3988, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 3989, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 3990, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 3991, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 3992, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 3993, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 3994, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 3995, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 3996, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 3997, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 3998, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 3999, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 4000, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 4001, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 4002, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 4003, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014288239181041718
Iteration: 4004, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 3.7348829209804535e-05
Iteration: 4005, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030273771844804287
Iteration: 4006, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002716127783060074
Collision detected, score: 0
Iteration: 4007, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00310795521363616
Iteration: 4008, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002749205566942692
Iteration: 4009, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006391205359250307
Iteration: 4010, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003953661769628525
Iteration: 4011, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006350154057145119
Iteration: 4012, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 4013, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 4014, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 4015, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 4016, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 4017, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 4018, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 4019, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 4020, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 4021, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 4022, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 4023, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 4024, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 4025, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 4026, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 4027, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 4028, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 4029, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 4030, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017030145972967148
Iteration: 4031, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0002404404804110527
Iteration: 4032, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0021265875548124313
Iteration: 4033, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026417854242026806
Collision detected, score: 0
Iteration: 4034, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.004013169091194868
Iteration: 4035, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0022076554596424103
Iteration: 4036, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006902886554598808
Iteration: 4037, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004047383088618517
Iteration: 4038, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00044811051338911057
Iteration: 4039, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 4040, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 4041, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 4042, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 4043, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 4044, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 4045, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 4046, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 4047, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 4048, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 4049, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 4050, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 4051, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 4052, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 4053, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 4054, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 4055, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 4056, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 4057, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005811928305774927
Iteration: 4058, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005306202452629805
Iteration: 4059, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003243167418986559
Iteration: 4060, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002700460609048605
Iteration: 4061, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003069089725613594
Iteration: 4062, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007004765793681145
Iteration: 4063, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008815964683890343
Iteration: 4064, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0052032130770385265
Iteration: 4065, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009080291725695133
Iteration: 4066, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010573633015155792
Iteration: 4067, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009188799187541008
Iteration: 4068, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007896583527326584
Iteration: 4069, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004616984166204929
Iteration: 4070, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007701746188104153
Iteration: 4071, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0074674636125564575
Iteration: 4072, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0073607261292636395
Iteration: 4073, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005127751734107733
Iteration: 4074, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006916834507137537
Iteration: 4075, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005121445283293724
Iteration: 4076, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036862753331661224
Iteration: 4077, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005083295051008463
Iteration: 4078, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006397668272256851
Iteration: 4079, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008646287024021149
Iteration: 4080, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011156878434121609
Iteration: 4081, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011767322197556496
Iteration: 4082, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007934793829917908
Iteration: 4083, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011533799581229687
Iteration: 4084, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009595289826393127
Iteration: 4085, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006091269664466381
Iteration: 4086, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376907996833324
Iteration: 4087, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004156854934990406
Iteration: 4088, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002552152145653963
Iteration: 4089, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009252435527741909
Iteration: 4090, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005119914188981056
Iteration: 4091, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008385032415390015
Iteration: 4092, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012974003329873085
Iteration: 4093, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004552231635898352
Iteration: 4094, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012579970061779022
Iteration: 4095, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006609419360756874
Iteration: 4096, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002535030245780945
Iteration: 4097, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005923283286392689
Iteration: 4098, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030206204392015934
Iteration: 4099, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001433027908205986
Iteration: 4100, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005236509256064892
Iteration: 4101, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025353049859404564
Iteration: 4102, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004689088091254234
Collision detected, score: 0
Iteration: 4103, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0034462008625268936
Iteration: 4104, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0056195687502622604
Iteration: 4105, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001492009498178959
Iteration: 4106, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007318748626857996
Iteration: 4107, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004039549268782139
Iteration: 4108, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4109, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4110, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4111, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4112, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4113, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4114, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4115, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4116, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4117, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4118, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4119, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4120, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4121, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4122, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4123, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4124, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4125, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4126, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018085958436131477
Iteration: 4127, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00239385524764657
Iteration: 4128, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013159485533833504
Iteration: 4129, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007709367200732231
Iteration: 4130, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00038112886250019073
Iteration: 4131, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010655978694558144
Iteration: 4132, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001663409173488617
Iteration: 4133, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0010825945064425468
Iteration: 4134, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023880023509263992
Iteration: 4135, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007982934825122356
Iteration: 4136, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0072815194725990295
Iteration: 4137, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002746433950960636
Iteration: 4138, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006395738571882248
Iteration: 4139, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007825393229722977
Iteration: 4140, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007216374855488539
Iteration: 4141, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00735319871455431
Perfoming an random action
Iteration: 4142, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011042574420571327
Iteration: 4143, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012796534225344658
Iteration: 4144, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009864830411970615
Iteration: 4145, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007025764323771
Iteration: 4146, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011101099662482738
Iteration: 4147, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011118464171886444
Iteration: 4148, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006595155224204063
Iteration: 4149, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006446302868425846
Iteration: 4150, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008051630109548569
Iteration: 4151, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0062572890892624855
Iteration: 4152, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0105711929500103
Iteration: 4153, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.015616134740412235
Iteration: 4154, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01226341538131237
Iteration: 4155, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008975603617727757
Iteration: 4156, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007060469128191471
Iteration: 4157, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003622957970947027
Iteration: 4158, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0078074634075164795
Iteration: 4159, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0005387896671891212
Iteration: 4160, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027883267030119896
Iteration: 4161, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005209129303693771
Iteration: 4162, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005552135407924652
Iteration: 4163, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00044373609125614166
Iteration: 4164, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037803854793310165
Iteration: 4165, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001929471269249916
Iteration: 4166, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0042164139449596405
Iteration: 4167, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0013938387855887413
Collision detected, score: 0
Iteration: 4168, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0028438465669751167
Iteration: 4169, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034107090905308723
Iteration: 4170, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003652325365692377
Iteration: 4171, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010268354788422585
Iteration: 4172, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007157893385738134
Iteration: 4173, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4174, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4175, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4176, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4177, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4178, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4179, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4180, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4181, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4182, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4183, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4184, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4185, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4186, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4187, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4188, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4189, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4190, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4191, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019315993413329124
Iteration: 4192, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018901647999882698
Iteration: 4193, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004452802240848541
Iteration: 4194, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005843071267008781
Iteration: 4195, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.3262338042259216e-05
Iteration: 4196, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000743567943572998
Iteration: 4197, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0003584194928407669
Iteration: 4198, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017990320920944214
Iteration: 4199, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003022518940269947
Iteration: 4200, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008430302143096924
Iteration: 4201, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00963389128446579
Iteration: 4202, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004122980870306492
Iteration: 4203, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00931660644710064
Iteration: 4204, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01165933720767498
Iteration: 4205, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009071690030395985
Iteration: 4206, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009079128503799438
Iteration: 4207, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009859040379524231
Iteration: 4208, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010249897837638855
Iteration: 4209, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0066459327936172485
Iteration: 4210, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0031372904777526855
Iteration: 4211, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007045144215226173
Iteration: 4212, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00848153792321682
Iteration: 4213, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008553079329431057
Iteration: 4214, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009096621535718441
Iteration: 4215, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005017060786485672
Iteration: 4216, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029446734115481377
Iteration: 4217, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007362673059105873
Iteration: 4218, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0002176780253648758
Iteration: 4219, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021590394899249077
Iteration: 4220, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002257397398352623
Iteration: 4221, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003007895313203335
Iteration: 4222, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011490434408187866
Iteration: 4223, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033168932422995567
Iteration: 4224, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00038223713636398315
Iteration: 4225, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011657830327749252
Iteration: 4226, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002127675339579582
Iteration: 4227, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017816899344325066
Iteration: 4228, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019431263208389282
Iteration: 4229, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0021154414862394333
Iteration: 4230, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0018828082829713821
Iteration: 4231, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0025083296932280064
Iteration: 4232, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004246523138135672
Iteration: 4233, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005533798597753048
Iteration: 4234, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007084779907017946
Iteration: 4235, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029272637329995632
Iteration: 4236, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005030396394431591
Collision detected, score: 0
Iteration: 4237, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.005527735687792301
Iteration: 4238, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004590381868183613
Iteration: 4239, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004736721981316805
Iteration: 4240, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006589720956981182
Iteration: 4241, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004203356336802244
Iteration: 4242, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4243, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4244, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4245, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4246, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4247, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4248, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4249, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4250, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4251, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4252, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4253, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4254, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4255, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4256, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4257, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Perfoming an random action
Iteration: 4258, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4259, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4260, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019503962248563766
Iteration: 4261, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012094099074602127
Iteration: 4262, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012727957218885422
Iteration: 4263, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00026383157819509506
Iteration: 4264, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009598284959793091
Iteration: 4265, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005308631807565689
Iteration: 4266, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002439369447529316
Iteration: 4267, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029760776087641716
Iteration: 4268, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004251241218298674
Iteration: 4269, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009385153651237488
Iteration: 4270, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008837942034006119
Iteration: 4271, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007109681610018015
Iteration: 4272, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00996675156056881
Iteration: 4273, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006624680012464523
Iteration: 4274, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00603124126791954
Iteration: 4275, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027878652326762676
Iteration: 4276, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006365327164530754
Iteration: 4277, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008420982398092747
Iteration: 4278, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006854395382106304
Iteration: 4279, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007233825512230396
Iteration: 4280, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011115306988358498
Iteration: 4281, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009523035027086735
Iteration: 4282, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011109433136880398
Iteration: 4283, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007789927069097757
Iteration: 4284, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007919718511402607
Iteration: 4285, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009635623544454575
Iteration: 4286, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009427351877093315
Iteration: 4287, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007515674456954002
Iteration: 4288, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003416488878428936
Iteration: 4289, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001492808572947979
Iteration: 4290, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019331835210323334
Iteration: 4291, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0013343794271349907
Iteration: 4292, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008878130465745926
Iteration: 4293, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002955777570605278
Iteration: 4294, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002013642340898514
Iteration: 4295, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001512572169303894
Iteration: 4296, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004151967354118824
Iteration: 4297, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014666514471173286
Perfoming an random action
Iteration: 4298, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0012590931728482246
Iteration: 4299, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0032593626528978348
Collision detected, score: 0
Iteration: 4300, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.001335417851805687
Iteration: 4301, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00269812298938632
Iteration: 4302, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002297417726367712
Iteration: 4303, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033188308589160442
Iteration: 4304, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004483395256102085
Iteration: 4305, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4306, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4307, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4308, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4309, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4310, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4311, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4312, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4313, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4314, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4315, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4316, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4317, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4318, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4319, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4320, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4321, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4322, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4323, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014446163550019264
Iteration: 4324, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001539016142487526
Iteration: 4325, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 9.498745203018188e-05
Iteration: 4326, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 1.9300729036331177e-05
Iteration: 4327, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 9.099394083023071e-05
Iteration: 4328, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0009527793154120445
Iteration: 4329, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009565697982907295
Iteration: 4330, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033964221365749836
Iteration: 4331, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028856140561401844
Iteration: 4332, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008353294804692268
Iteration: 4333, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007744165137410164
Iteration: 4334, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0066691553220152855
Iteration: 4335, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006973415147513151
Iteration: 4336, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006925489753484726
Iteration: 4337, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008532910607755184
Iteration: 4338, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0050951773300766945
Iteration: 4339, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009046424180269241
Iteration: 4340, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010527847334742546
Iteration: 4341, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01033387053757906
Iteration: 4342, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01155012659728527
Iteration: 4343, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01309945061802864
Iteration: 4344, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009543629363179207
Iteration: 4345, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009386466816067696
Iteration: 4346, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010733471252024174
Iteration: 4347, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009445496834814548
Iteration: 4348, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009156962856650352
Iteration: 4349, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008399316109716892
Iteration: 4350, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010011116042733192
Iteration: 4351, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005752504803240299
Iteration: 4352, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002997199073433876
Iteration: 4353, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008761808276176453
Iteration: 4354, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0029293568804860115
Iteration: 4355, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007464857771992683
Iteration: 4356, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0015788264572620392
Iteration: 4357, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0010858755558729172
Iteration: 4358, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0002904832363128662
Iteration: 4359, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0016621891409158707
Iteration: 4360, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006925213150680065
Iteration: 4361, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011382251977920532
Iteration: 4362, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0036766519770026207
Iteration: 4363, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0035434728488326073
Perfoming an random action
Iteration: 4364, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002120823599398136
Iteration: 4365, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005016922019422054
Iteration: 4366, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004670863039791584
Iteration: 4367, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012554991990327835
Iteration: 4368, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031330971978604794
Iteration: 4369, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00719304196536541
Iteration: 4370, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0013337470591068268
Iteration: 4371, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030792877078056335
Collision detected, score: 0
Iteration: 4372, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.006257943343371153
Iteration: 4373, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010882028378546238
Iteration: 4374, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00639713928103447
Iteration: 4375, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00835429411381483
Iteration: 4376, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006927520968019962
Iteration: 4377, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4378, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4379, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4380, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4381, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4382, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4383, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Perfoming an random action
Iteration: 4384, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4385, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4386, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0011615874245762825
Iteration: 4387, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0004676077514886856
Iteration: 4388, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005443114787340164
Iteration: 4389, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0013387836515903473
Iteration: 4390, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036139492876827717
Iteration: 4391, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025397976860404015
Iteration: 4392, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005442820489406586
Iteration: 4393, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005371787119656801
Iteration: 4394, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005278109107166529
Iteration: 4395, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015147393569350243
Iteration: 4396, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005170917138457298
Iteration: 4397, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004014779347926378
Iteration: 4398, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006236575543880463
Iteration: 4399, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029436033219099045
Iteration: 4400, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00012813322246074677
Iteration: 4401, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004357486963272095
Iteration: 4402, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029918495565652847
Iteration: 4403, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002997893840074539
Iteration: 4404, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004828365053981543
Iteration: 4405, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004652684088796377
Iteration: 4406, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014580730348825455
Iteration: 4407, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00242576003074646
Iteration: 4408, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033580120652914047
Iteration: 4409, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034697363153100014
Iteration: 4410, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002067049965262413
Iteration: 4411, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017796000465750694
Iteration: 4412, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002309237141162157
Iteration: 4413, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022104671224951744
Iteration: 4414, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002855194266885519
Iteration: 4415, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0034792497754096985
Iteration: 4416, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00451842974871397
Iteration: 4417, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007654695771634579
Iteration: 4418, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018459735438227654
Iteration: 4419, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00259533803910017
Iteration: 4420, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037606600672006607
Iteration: 4421, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002787924837321043
Iteration: 4422, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009488633833825588
Iteration: 4423, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008093158714473248
Iteration: 4424, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008330337703227997
Iteration: 4425, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008372236974537373
Iteration: 4426, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024690404534339905
Iteration: 4427, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006986072286963463
Iteration: 4428, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0035910112783312798
Iteration: 4429, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0065054623410105705
Iteration: 4430, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008123398758471012
Iteration: 4431, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002091354690492153
Iteration: 4432, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002937980927526951
Iteration: 4433, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004279185086488724
Iteration: 4434, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004547432530671358
Iteration: 4435, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005282664205878973
Iteration: 4436, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007168014533817768
Iteration: 4437, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009289246052503586
Iteration: 4438, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012578288093209267
Iteration: 4439, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007501150947064161
Iteration: 4440, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00020960159599781036
Iteration: 4441, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0021604839712381363
Iteration: 4442, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004346898756921291
Iteration: 4443, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002674963790923357
Iteration: 4444, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0049538202583789825
Iteration: 4445, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002332097850739956
Iteration: 4446, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019162911921739578
Iteration: 4447, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0016361111775040627
Collision detected, score: 0
Iteration: 4448, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0009976504370570183
Iteration: 4449, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002524709329009056
Iteration: 4450, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00502367690205574
Iteration: 4451, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029511507600545883
Iteration: 4452, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034987558610737324
Iteration: 4453, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4454, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4455, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4456, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4457, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4458, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4459, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4460, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4461, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4462, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4463, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4464, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4465, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4466, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4467, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4468, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Perfoming an random action
Iteration: 4469, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4470, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4471, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002257818356156349
Iteration: 4472, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001129787415266037
Iteration: 4473, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009863264858722687
Iteration: 4474, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005442332476377487
Iteration: 4475, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008894139900803566
Iteration: 4476, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0001153675839304924
Iteration: 4477, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028016911819577217
Iteration: 4478, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004283212590962648
Iteration: 4479, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004473614040762186
Iteration: 4480, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010275587439537048
Iteration: 4481, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010818338952958584
Iteration: 4482, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006354875862598419
Iteration: 4483, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011306574568152428
Iteration: 4484, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005020225886255503
Iteration: 4485, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005041412077844143
Iteration: 4486, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005343813914805651
Iteration: 4487, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004638885613530874
Iteration: 4488, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0063815051689744
Iteration: 4489, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0021391669288277626
Iteration: 4490, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004348919726908207
Iteration: 4491, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004975103307515383
Iteration: 4492, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006127529311925173
Iteration: 4493, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009722327813506126
Iteration: 4494, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005578839220106602
Iteration: 4495, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00451743183657527
Iteration: 4496, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015257708728313446
Iteration: 4497, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004908342845737934
Iteration: 4498, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004973147064447403
Iteration: 4499, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007268935441970825
Iteration: 4500, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016703438013792038
Iteration: 4501, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006716689094901085
Iteration: 4502, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027753617614507675
Iteration: 4503, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000905442051589489
Iteration: 4504, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00019396282732486725
Iteration: 4505, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020683705806732178
Iteration: 4506, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008873473852872849
Iteration: 4507, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0044722529128193855
Iteration: 4508, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002248791977763176
Iteration: 4509, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010151974856853485
Iteration: 4510, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00033659860491752625
Iteration: 4511, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0003766091540455818
Iteration: 4512, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.004873785190284252
Iteration: 4513, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.005150499753654003
Iteration: 4514, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0032840194180607796
Iteration: 4515, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0066429078578948975
Iteration: 4516, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.004755987785756588
Collision detected, score: 0
Iteration: 4517, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.004466566722840071
Iteration: 4518, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003646551165729761
Iteration: 4519, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 6.422027945518494e-05
Iteration: 4520, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003690199926495552
Iteration: 4521, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020534228533506393
Iteration: 4522, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 4523, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 4524, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 4525, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 4526, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 4527, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 4528, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 4529, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 4530, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 4531, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 4532, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 4533, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 4534, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 4535, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 4536, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 4537, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 4538, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 4539, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 4540, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000823264941573143
Iteration: 4541, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002522825263440609
Iteration: 4542, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036624111235141754
Collision detected, score: 0
Iteration: 4543, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002866821363568306
Iteration: 4544, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002279665321111679
Iteration: 4545, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006620822474360466
Iteration: 4546, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005065648816525936
Iteration: 4547, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007613599300384521
Iteration: 4548, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 4549, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 4550, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 4551, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 4552, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 4553, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 4554, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 4555, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 4556, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 4557, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 4558, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 4559, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 4560, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 4561, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 4562, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 4563, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 4564, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 4565, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 4566, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007727397605776787
Iteration: 4567, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002561999950557947
Iteration: 4568, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004544357769191265
Collision detected, score: 0
Iteration: 4569, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0030289464630186558
Iteration: 4570, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001087326556444168
Iteration: 4571, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008073925040662289
Iteration: 4572, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007212257012724876
Iteration: 4573, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00616328464820981
Iteration: 4574, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 4575, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 4576, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 4577, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 4578, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 4579, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 4580, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 4581, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 4582, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 4583, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 4584, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 4585, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 4586, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 4587, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 4588, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 4589, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 4590, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 4591, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 4592, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014221081510186195
Iteration: 4593, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 6.419606506824493e-06
Iteration: 4594, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0031059780158102512
Iteration: 4595, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027881618589162827
Collision detected, score: 0
Iteration: 4596, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.003150020260363817
Iteration: 4597, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027554836124181747
Iteration: 4598, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006346030160784721
Iteration: 4599, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0040787640027701855
Iteration: 4600, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006504708901047707
Iteration: 4601, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014296472072601318
Iteration: 4602, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00022489670664072037
Iteration: 4603, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0010067196562886238
Iteration: 4604, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006234664469957352
Iteration: 4605, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034183328971266747
Iteration: 4606, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009657908231019974
Iteration: 4607, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001625504344701767
Iteration: 4608, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0026838290505111217
Iteration: 4609, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005176476668566465
Iteration: 4610, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005650958977639675
Iteration: 4611, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00563843734562397
Iteration: 4612, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016668401658535004
Iteration: 4613, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0011343983933329582
Iteration: 4614, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008991323411464691
Iteration: 4615, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0006038779392838478
Iteration: 4616, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.000574043020606041
Iteration: 4617, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0011950712651014328
Iteration: 4618, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015909168869256973
Iteration: 4619, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014744261279702187
Iteration: 4620, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00016565527766942978
Iteration: 4621, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033555207774043083
Iteration: 4622, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022200122475624084
Collision detected, score: 0
Iteration: 4623, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0019000954926013947
Iteration: 4624, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002608224283903837
Iteration: 4625, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0033899489790201187
Iteration: 4626, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004401151556521654
Iteration: 4627, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -5.151145160198212e-05
Iteration: 4628, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 4629, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 4630, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 4631, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 4632, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 4633, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 4634, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 4635, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 4636, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 4637, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 4638, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 4639, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 4640, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 4641, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 4642, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 4643, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 4644, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 4645, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 4646, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007004896178841591
Iteration: 4647, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002473248168826103
Iteration: 4648, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003426752984523773
Collision detected, score: 0
Iteration: 4649, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0018147733062505722
Iteration: 4650, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00041722413152456284
Iteration: 4651, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007379784248769283
Iteration: 4652, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0064760250970721245
Iteration: 4653, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006472643464803696
Iteration: 4654, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4655, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4656, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4657, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4658, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4659, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4660, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4661, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4662, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4663, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4664, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4665, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4666, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4667, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4668, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4669, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4670, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4671, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4672, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017618490383028984
Iteration: 4673, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0020106183364987373
Iteration: 4674, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008164104074239731
Iteration: 4675, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0004006503149867058
Iteration: 4676, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00014816131442785263
Iteration: 4677, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0009333314374089241
Iteration: 4678, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00016709044575691223
Iteration: 4679, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015607001259922981
Iteration: 4680, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00315923523157835
Iteration: 4681, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006768023129552603
Iteration: 4682, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008942694403231144
Iteration: 4683, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004152470733970404
Iteration: 4684, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007379788439720869
Iteration: 4685, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011045056395232677
Iteration: 4686, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010302763432264328
Iteration: 4687, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008749195374548435
Iteration: 4688, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012199628166854382
Iteration: 4689, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.015033886767923832
Iteration: 4690, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012012293562293053
Iteration: 4691, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00991767831146717
Iteration: 4692, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.015281109139323235
Iteration: 4693, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010973306372761726
Iteration: 4694, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01195746473968029
Iteration: 4695, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011597637087106705
Iteration: 4696, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007932020351290703
Iteration: 4697, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005301613360643387
Iteration: 4698, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0075937844812870026
Iteration: 4699, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00571359833702445
Iteration: 4700, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0064407093450427055
Iteration: 4701, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0004330510273575783
Iteration: 4702, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0018784645944833755
Iteration: 4703, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0028677787631750107
Iteration: 4704, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0009476412087678909
Iteration: 4705, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0015204334631562233
Iteration: 4706, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0021277591586112976
Iteration: 4707, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005981829017400742
Iteration: 4708, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00037673115730285645
Iteration: 4709, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005766274407505989
Iteration: 4710, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00016287807375192642
Iteration: 4711, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0035294238477945328
Iteration: 4712, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004375508055090904
Iteration: 4713, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005825775675475597
Iteration: 4714, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00027668289840221405
Iteration: 4715, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003562711179256439
Iteration: 4716, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002115987241268158
Iteration: 4717, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002545170020312071
Iteration: 4718, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004190476145595312
Collision detected, score: 0
Iteration: 4719, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: -0.001319204457104206
Iteration: 4720, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0061850883066654205
Iteration: 4721, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00810806080698967
Iteration: 4722, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009553439915180206
Iteration: 4723, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005901031196117401
Iteration: 4724, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4725, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4726, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4727, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4728, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4729, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4730, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4731, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4732, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4733, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4734, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4735, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4736, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4737, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4738, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4739, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4740, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4741, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4742, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018085958436131477
Iteration: 4743, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00239385524764657
Iteration: 4744, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013159485533833504
Iteration: 4745, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000352335162460804
Iteration: 4746, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0003831535577774048
Iteration: 4747, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014431234449148178
Iteration: 4748, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014532459899783134
Iteration: 4749, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0021700095385313034
Iteration: 4750, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017618434503674507
Iteration: 4751, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009464619681239128
Iteration: 4752, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0092843659222126
Iteration: 4753, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007794639095664024
Iteration: 4754, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009552659466862679
Iteration: 4755, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006285451352596283
Iteration: 4756, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0072635808028280735
Iteration: 4757, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0067253015004098415
Iteration: 4758, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009472774341702461
Iteration: 4759, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014205122366547585
Iteration: 4760, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012669587507843971
Iteration: 4761, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01238847803324461
Iteration: 4762, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011755362153053284
Iteration: 4763, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014633923768997192
Iteration: 4764, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01099509559571743
Iteration: 4765, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01240618247538805
Iteration: 4766, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011316288262605667
Iteration: 4767, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0064905257895588875
Iteration: 4768, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006403840612620115
Iteration: 4769, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00545719638466835
Iteration: 4770, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006573548074811697
Iteration: 4771, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018339604139328003
Iteration: 4772, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.004266410134732723
Iteration: 4773, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.004809673875570297
Iteration: 4774, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0008657844737172127
Iteration: 4775, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.000914672389626503
Iteration: 4776, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014113672077655792
Iteration: 4777, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00206105038523674
Iteration: 4778, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0015003420412540436
Iteration: 4779, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00022316910326480865
Iteration: 4780, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0016909567639231682
Iteration: 4781, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0004104413092136383
Iteration: 4782, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006983330473303795
Iteration: 4783, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028956513851881027
Iteration: 4784, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002037694677710533
Iteration: 4785, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0013280902057886124
Iteration: 4786, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -1.4977529644966125e-05
Collision detected, score: 0
Iteration: 4787, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0020430535078048706
Iteration: 4788, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003893272951245308
Iteration: 4789, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0036807656288146973
Iteration: 4790, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00812820065766573
Iteration: 4791, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005301917903125286
Iteration: 4792, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 4793, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 4794, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 4795, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 4796, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 4797, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 4798, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 4799, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 4800, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 4801, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 4802, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 4803, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 4804, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 4805, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 4806, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 4807, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 4808, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 4809, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 4810, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006341999396681786
Iteration: 4811, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002619645558297634
Iteration: 4812, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0042110285721719265
Collision detected, score: 0
Iteration: 4813, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.002815762534737587
Iteration: 4814, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012250114232301712
Iteration: 4815, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007226733490824699
Iteration: 4816, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004312742967158556
Iteration: 4817, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005336988251656294
Iteration: 4818, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4819, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4820, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4821, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4822, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4823, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4824, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4825, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4826, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4827, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4828, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4829, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4830, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4831, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4832, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4833, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4834, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Perfoming an random action
Iteration: 4835, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4836, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001959453336894512
Iteration: 4837, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002097048796713352
Iteration: 4838, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007327813655138016
Iteration: 4839, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0005528116598725319
Iteration: 4840, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00026099104434251785
Iteration: 4841, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012899916619062424
Iteration: 4842, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007053418084979057
Iteration: 4843, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00109030120074749
Iteration: 4844, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0027513718232512474
Iteration: 4845, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00669487752020359
Iteration: 4846, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009118537418544292
Iteration: 4847, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022719274275004864
Iteration: 4848, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008343188092112541
Iteration: 4849, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010580805130302906
Iteration: 4850, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010687882080674171
Iteration: 4851, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008401243947446346
Iteration: 4852, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011442473158240318
Iteration: 4853, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014054904691874981
Iteration: 4854, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011929254978895187
Iteration: 4855, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009998267516493797
Iteration: 4856, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014690259471535683
Iteration: 4857, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011464320123195648
Iteration: 4858, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011623683385550976
Iteration: 4859, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011926233768463135
Iteration: 4860, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008062412962317467
Iteration: 4861, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006584312301129103
Iteration: 4862, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006208452396094799
Iteration: 4863, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005165598355233669
Iteration: 4864, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006020881235599518
Iteration: 4865, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0012420592829585075
Iteration: 4866, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0028610583394765854
Iteration: 4867, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002317129634320736
Iteration: 4868, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007663331925868988
Iteration: 4869, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0033247070387005806
Iteration: 4870, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0001740073785185814
Iteration: 4871, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002008887007832527
Iteration: 4872, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001825760118663311
Iteration: 4873, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0022609271109104156
Iteration: 4874, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00034880079329013824
Iteration: 4875, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0028928127139806747
Iteration: 4876, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0022411723621189594
Iteration: 4877, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003824160434305668
Iteration: 4878, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0013201432302594185
Iteration: 4879, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0036467434838414192
Iteration: 4880, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000860217958688736
Collision detected, score: 0
Iteration: 4881, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.0035378187894821167
Iteration: 4882, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017696768045425415
Iteration: 4883, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00045694224536418915
Iteration: 4884, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006848825141787529
Iteration: 4885, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004301042761653662
Iteration: 4886, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014210222288966179
Iteration: 4887, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043724197894334793
Iteration: 4888, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003449292853474617
Iteration: 4889, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0018552988767623901
Iteration: 4890, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002821379341185093
Iteration: 4891, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005376496817916632
Iteration: 4892, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005158834625035524
Iteration: 4893, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859615746885538
Iteration: 4894, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002437031362205744
Iteration: 4895, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0014892090111970901
Iteration: 4896, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002252996899187565
Iteration: 4897, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 4.48739156126976e-05
Iteration: 4898, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012874118983745575
Iteration: 4899, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006162431091070175
Iteration: 4900, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0019314270466566086
Iteration: 4901, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.001413922756910324
Iteration: 4902, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002123415470123291
Iteration: 4903, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028987512923777103
Iteration: 4904, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006830785423517227
Iteration: 4905, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003166177310049534
Iteration: 4906, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025839051231741905
Collision detected, score: 0
Iteration: 4907, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.00176988635212183
Iteration: 4908, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00032860226929187775
Iteration: 4909, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005506264045834541
Iteration: 4910, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005922914016991854
Iteration: 4911, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0006377594545483589
Iteration: 4912, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003378656692802906
Iteration: 4913, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0014798445627093315
Iteration: 4914, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00035334378480911255
Iteration: 4915, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017679156735539436
Iteration: 4916, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002086191438138485
Iteration: 4917, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005170070566236973
Iteration: 4918, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043724654242396355
Iteration: 4919, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003417236264795065
Iteration: 4920, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037532420828938484
Iteration: 4921, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0047527640126645565
Iteration: 4922, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003918151371181011
Iteration: 4923, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0032853325828909874
Iteration: 4924, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004802387673407793
Iteration: 4925, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015577496960759163
Iteration: 4926, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001332668587565422
Iteration: 4927, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00010597985237836838
Iteration: 4928, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002928490284830332
Iteration: 4929, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030294503085315228
Iteration: 4930, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005911155603826046
Iteration: 4931, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0037833843380212784
Iteration: 4932, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0029745884239673615
Iteration: 4933, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002185211516916752
Iteration: 4934, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038695083931088448
Iteration: 4935, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007311239838600159
Iteration: 4936, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015308540314435959
Iteration: 4937, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00684155011549592
Iteration: 4938, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008907406590878963
Iteration: 4939, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010147756896913052
Iteration: 4940, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011029121465981007
Iteration: 4941, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00793551467359066
Iteration: 4942, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00599175039678812
Iteration: 4943, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0076528266072273254
Iteration: 4944, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006160683464258909
Iteration: 4945, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009118422865867615
Iteration: 4946, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00681600347161293
Iteration: 4947, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007086819037795067
Iteration: 4948, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0024401829577982426
Iteration: 4949, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002181929536163807
Iteration: 4950, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034010489471256733
Iteration: 4951, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0039182077161967754
Iteration: 4952, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007164007052779198
Iteration: 4953, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005601457320153713
Iteration: 4954, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038710790686309338
Iteration: 4955, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0017456458881497383
Iteration: 4956, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004665033891797066
Iteration: 4957, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005335272755473852
Iteration: 4958, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005694874096661806
Iteration: 4959, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032484810799360275
Iteration: 4960, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004190795589238405
Iteration: 4961, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002517972607165575
Iteration: 4962, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007336719892919064
Iteration: 4963, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004378508310765028
Iteration: 4964, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006003935355693102
Iteration: 4965, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0033429311588406563
Iteration: 4966, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004364594351500273
Iteration: 4967, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00474231131374836
Iteration: 4968, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.000929536297917366
Iteration: 4969, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.003087112680077553
Collision detected, score: 0
Iteration: 4970, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 6.264355033636093e-05
Iteration: 4971, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003025466576218605
Iteration: 4972, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003919861279428005
Iteration: 4973, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003944041673094034
Iteration: 4974, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004824228584766388
Iteration: 4975, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 4976, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 4977, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 4978, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 4979, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 4980, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 4981, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 4982, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 4983, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 4984, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 4985, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 4986, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 4987, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 4988, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 4989, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 4990, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 4991, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 4992, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 4993, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002001662738621235
Iteration: 4994, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010593878105282784
Iteration: 4995, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007365886121988297
Iteration: 4996, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0001870039850473404
Iteration: 4997, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00026066601276397705
Iteration: 4998, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.000898534432053566
Iteration: 4999, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0023226384073495865
Iteration: 5000, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037346119061112404
Iteration: 5001, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0037356908433139324
Iteration: 5002, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007639684248715639
Perfoming an random action
Iteration: 5003, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007019295357167721
Iteration: 5004, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006833411753177643
Iteration: 5005, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007895015180110931
Iteration: 5006, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005437465850263834
Iteration: 5007, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007223308086395264
Iteration: 5008, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002781820483505726
Iteration: 5009, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005455043166875839
Iteration: 5010, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0051775285974144936
Iteration: 5011, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0032776780426502228
Iteration: 5012, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004561509005725384
Iteration: 5013, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0052076843567192554
Iteration: 5014, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005289811175316572
Iteration: 5015, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005671836901456118
Iteration: 5016, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006390717811882496
Iteration: 5017, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004314810037612915
Iteration: 5018, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0043705725111067295
Iteration: 5019, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006268656346946955
Iteration: 5020, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007432871963828802
Iteration: 5021, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004408782348036766
Iteration: 5022, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004486548714339733
Iteration: 5023, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005008013918995857
Iteration: 5024, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007953613996505737
Iteration: 5025, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012722323648631573
Iteration: 5026, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009785927832126617
Iteration: 5027, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.014303907752037048
Iteration: 5028, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008779584430158138
Iteration: 5029, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012358073145151138
Iteration: 5030, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0005458472296595573
Iteration: 5031, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00028607528656721115
Collision detected, score: 0
Iteration: 5032, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.001192745752632618
Iteration: 5033, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004585748538374901
Iteration: 5034, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007938234135508537
Iteration: 5035, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001188153401017189
Iteration: 5036, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010520759969949722
Iteration: 5037, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002407754771411419
Iteration: 5038, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0017140824347734451
Iteration: 5039, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016389107331633568
Iteration: 5040, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0014947187155485153
Iteration: 5041, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001966129057109356
Iteration: 5042, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004209826700389385
Iteration: 5043, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004745318088680506
Iteration: 5044, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003355017863214016
Iteration: 5045, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004265457391738892
Iteration: 5046, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004399097058922052
Iteration: 5047, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004322832450270653
Iteration: 5048, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0019652200862765312
Iteration: 5049, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001681785099208355
Iteration: 5050, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0006626388058066368
Iteration: 5051, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003104171250015497
Iteration: 5052, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0034972475841641426
Iteration: 5053, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008184014819562435
Iteration: 5054, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00528718251734972
Iteration: 5055, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0049131750129163265
Iteration: 5056, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0041222223080694675
Iteration: 5057, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003939829766750336
Iteration: 5058, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006071928888559341
Iteration: 5059, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00020703673362731934
Iteration: 5060, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0029983981512486935
Iteration: 5061, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0058658309280872345
Iteration: 5062, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004193752072751522
Iteration: 5063, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005405616480857134
Iteration: 5064, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0062477076426148415
Iteration: 5065, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005876899231225252
Iteration: 5066, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004070288967341185
Iteration: 5067, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0090511878952384
Iteration: 5068, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00891952309757471
Iteration: 5069, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00887525174766779
Iteration: 5070, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004479713272303343
Iteration: 5071, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008361466228961945
Iteration: 5072, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007361134979873896
Iteration: 5073, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0068563977256417274
Iteration: 5074, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010763579048216343
Iteration: 5075, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011373904533684254
Iteration: 5076, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012878555804491043
Iteration: 5077, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.013437449000775814
Iteration: 5078, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010703702457249165
Iteration: 5079, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010030663572251797
Iteration: 5080, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009885113686323166
Iteration: 5081, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007873962633311749
Iteration: 5082, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.011192661710083485
Iteration: 5083, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008770612999796867
Iteration: 5084, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008477731607854366
Iteration: 5085, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009825630113482475
Iteration: 5086, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004654246848076582
Iteration: 5087, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001947704702615738
Iteration: 5088, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0020117992535233498
Iteration: 5089, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0007153842598199844
Iteration: 5090, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003511602059006691
Iteration: 5091, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0009700413793325424
Iteration: 5092, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003942111041396856
Iteration: 5093, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.002507617697119713
Iteration: 5094, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0023031141608953476
Iteration: 5095, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0014161542057991028
Iteration: 5096, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0040381187573075294
Iteration: 5097, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0016860133036971092
Iteration: 5098, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0025965003296732903
Iteration: 5099, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0003659296780824661
Iteration: 5100, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0048402403481304646
Iteration: 5101, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.005604798439890146
Iteration: 5102, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010215220972895622
Collision detected, score: 0
Iteration: 5103, learning state: observe, epsilon: 0.0075, did jump: No, reward: -1, Q_max: 0.009726528078317642
Iteration: 5104, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.01087359432131052
Iteration: 5105, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008167280815541744
Iteration: 5106, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007927103899419308
Iteration: 5107, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006118884775787592
Iteration: 5108, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 5109, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 5110, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 5111, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 5112, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 5113, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 5114, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 5115, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 5116, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 5117, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 5118, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 5119, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 5120, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 5121, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 5122, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 5123, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 5124, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 5125, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 5126, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.002003614790737629
Iteration: 5127, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001372823491692543
Iteration: 5128, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.001245127059519291
Iteration: 5129, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00023598037660121918
Iteration: 5130, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0007955729961395264
Iteration: 5131, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0010458547621965408
Iteration: 5132, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.002171013504266739
Iteration: 5133, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0030071535147726536
Iteration: 5134, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004069356247782707
Iteration: 5135, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008268484845757484
Iteration: 5136, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00685925455763936
Iteration: 5137, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005067724268883467
Iteration: 5138, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00752774253487587
Iteration: 5139, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00507446750998497
Iteration: 5140, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005798135418444872
Iteration: 5141, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018322393298149109
Iteration: 5142, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004256551619619131
Iteration: 5143, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0012001991271972656
Iteration: 5144, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007633636239916086
Iteration: 5145, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.006367849186062813
Iteration: 5146, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0008218623697757721
Iteration: 5147, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00449281046167016
Iteration: 5148, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036760997027158737
Iteration: 5149, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00203657615929842
Iteration: 5150, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0021610548719763756
Iteration: 5151, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0012463806197047234
Iteration: 5152, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0022111902944743633
Iteration: 5153, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004783819429576397
Iteration: 5154, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.009452219121158123
Iteration: 5155, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.007523607462644577
Iteration: 5156, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.003598138689994812
Iteration: 5157, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00911859329789877
Iteration: 5158, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.012671586126089096
Iteration: 5159, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006195705384016037
Iteration: 5160, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008542395196855068
Iteration: 5161, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005994957406073809
Iteration: 5162, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004246764816343784
Iteration: 5163, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00520748645067215
Iteration: 5164, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004346874076873064
Iteration: 5165, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00971374474465847
Iteration: 5166, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004623280838131905
Iteration: 5167, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.010291032493114471
Iteration: 5168, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.010252485051751137
Iteration: 5169, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.008281056769192219
Iteration: 5170, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030276202596724033
Iteration: 5171, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0003811884671449661
Iteration: 5172, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00026661716401576996
Iteration: 5173, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0036493930965662003
Iteration: 5174, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0024159839376807213
Iteration: 5175, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.010220588184893131
Iteration: 5176, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.007729632314294577
Collision detected, score: 0
Iteration: 5177, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: -1, Q_max: 0.007821213454008102
Iteration: 5178, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005859138444066048
Iteration: 5179, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0018272409215569496
Iteration: 5180, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0007247216999530792
Iteration: 5181, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0038909786380827427
Iteration: 5182, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0009373389184474945
Iteration: 5183, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005003664642572403
Iteration: 5184, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004594935104250908
Iteration: 5185, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0028003468178212643
Iteration: 5186, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008488083258271217
Iteration: 5187, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0010890141129493713
Iteration: 5188, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0030139251612126827
Iteration: 5189, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.005339952185750008
Iteration: 5190, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004774799104779959
Iteration: 5191, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0021045105531811714
Iteration: 5192, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0023565771989524364
Iteration: 5193, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.0011561401188373566
Iteration: 5194, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0008004233241081238
Iteration: 5195, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.002270813100039959
Iteration: 5196, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0022749993950128555
Iteration: 5197, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0005448758602142334
Iteration: 5198, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: -0.00043363217264413834
Iteration: 5199, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -1.911912113428116e-05
Iteration: 5200, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015425654128193855
Iteration: 5201, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.0015769768506288528
Iteration: 5202, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.00037432555109262466
Iteration: 5203, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.00011356081813573837
Iteration: 5204, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0017800955101847649
Iteration: 5205, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: -0.0006559258326888084
Iteration: 5206, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003265207167714834
Iteration: 5207, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.004031477030366659
Iteration: 5208, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.003701810259371996
Iteration: 5209, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.004984279628843069
Iteration: 5210, learning state: observe, epsilon: 0.0075, did jump: No, reward: 0.1, Q_max: 0.006530468817800283
Iteration: 5211, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0028331982903182507
Iteration: 5212, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.0020846985280513763
Iteration: 5213, learning state: observe, epsilon: 0.0075, did jump: Yes, reward: 0.1, Q_max: 0.00355606060475111
